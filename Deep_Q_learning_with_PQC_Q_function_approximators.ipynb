{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirhoseinaghaei/Research_Simulation/blob/main/Deep_Q_learning_with_PQC_Q_function_approximators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9x7AMJTsTYL",
        "outputId": "90ae2666-5f06-4136-92bb-fec63f7777a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n",
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Research_Simmulation\n"
          ]
        }
      ],
      "source": [
        "!fusermount -u drive\n",
        "!google-drive-ocamlfuse drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')\n",
        "%cd gdrive/MyDrive/Research_Simmulation/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9B-DI1DsqUU"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.7.0\n",
        "!pip install tensorflow-quantum==0.7.2\n",
        "!pip install gym==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D4W4zrSzsyNG"
      },
      "outputs": [],
      "source": [
        "# Update package resources to account for version changes.\n",
        "import importlib, pkg_resources\n",
        "importlib.reload(pkg_resources)\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "import gym, cirq, sympy\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from collections import deque, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9uGLwLTCtEjP"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Rescaling(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Rescaling, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=tf.ones(shape=(1,input_dim)), dtype=\"float32\",\n",
        "            trainable=True, name=\"obs-weights\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.math.multiply((inputs+1)/2, tf.repeat(self.w,repeats=tf.shape(inputs)[0],axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0NminpPUtpXj"
      },
      "outputs": [],
      "source": [
        "n_qubits = 5 # Dimension of the state vectors in CartPole\n",
        "n_layers = 5 # Number of layers in the PQC\n",
        "n_actions = 2 # Number of actions in CartPole\n",
        "\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "ops = [cirq.Z(q) for q in qubits]\n",
        "observables = [ops[0]*ops[1], ops[2]*ops[3]] # Z_0*Z_1 for action 0 and Z_2*Z_3 for action 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v8KRsmhGuM8a"
      },
      "outputs": [],
      "source": [
        "def one_qubit_rotation(qubit, symbols):\n",
        "    \"\"\"\n",
        "    Returns Cirq gates that apply a rotation of the bloch sphere about the X,\n",
        "    Y and Z axis, specified by the values in `symbols`.\n",
        "    \"\"\"\n",
        "    return [cirq.rx(symbols[0])(qubit),\n",
        "            cirq.ry(symbols[1])(qubit),\n",
        "            cirq.rz(symbols[2])(qubit)]\n",
        "\n",
        "def entangling_layer(qubits):\n",
        "    \"\"\"\n",
        "    Returns a layer of CZ entangling gates on `qubits` (arranged in a circular topology).\n",
        "    \"\"\"\n",
        "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
        "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
        "    return cz_ops\n",
        "def generate_circuit(qubits, n_layers):\n",
        "    \"\"\"Prepares a data re-uploading circuit on `qubits` with `n_layers` layers.\"\"\"\n",
        "    # Number of qubits\n",
        "    n_qubits = len(qubits)\n",
        "    \n",
        "    # Sympy symbols for variational angles\n",
        "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
        "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
        "    \n",
        "    # Sympy symbols for encoding angles\n",
        "    inputs = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_qubits})')\n",
        "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
        "    \n",
        "    # Define circuit\n",
        "    circuit = cirq.Circuit()\n",
        "    for l in range(n_layers):\n",
        "        # Variational layer\n",
        "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
        "        circuit += entangling_layer(qubits)\n",
        "        # Encoding layer\n",
        "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
        "\n",
        "    # Last varitional layer\n",
        "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i,q in enumerate(qubits))\n",
        "    \n",
        "    return circuit, list(params.flat), list(inputs.flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n8sLi72juC_M"
      },
      "outputs": [],
      "source": [
        "class ReUploadingPQC(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Performs the transformation (s_1, ..., s_d) -> (theta_1, ..., theta_N, lmbd[1][1]s_1, ..., lmbd[1][M]s_1,\n",
        "        ......., lmbd[d][1]s_d, ..., lmbd[d][M]s_d) for d=input_dim, N=theta_dim and M=n_layers.\n",
        "    An activation function from tf.keras.activations, specified by `activation` ('linear' by default) is\n",
        "        then applied to all lmbd[i][j]s_i.\n",
        "    All angles are finally permuted to follow the alphabetical order of their symbol names, as processed\n",
        "        by the ControlledPQC.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
        "        super(ReUploadingPQC, self).__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.n_qubits = len(qubits)\n",
        "\n",
        "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
        "\n",
        "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
        "        self.theta = tf.Variable(\n",
        "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
        "            trainable=True, name=\"thetas\"\n",
        "        )\n",
        "        \n",
        "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
        "        self.lmbd = tf.Variable(\n",
        "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
        "        )\n",
        "        \n",
        "        # Define explicit symbol order.\n",
        "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
        "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
        "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)        \n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs[0] = encoding data for the state.\n",
        "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
        "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
        "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
        "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
        "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
        "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
        "\n",
        "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
        "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
        "        \n",
        "        return self.computation_layer([tiled_up_circuits, joined_vars])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r1uFD2y4t35N"
      },
      "outputs": [],
      "source": [
        "def generate_model_Qlearning(qubits, n_layers, n_actions, observables, target):\n",
        "    \"\"\"Generates a Keras model for a data re-uploading PQC Q-function approximator.\"\"\"\n",
        "\n",
        "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
        "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables, activation='tanh')([input_tensor])\n",
        "    process = tf.keras.Sequential([Rescaling(len(observables))], name=target*\"Target\"+\"Q-values\")\n",
        "    Q_values = process(re_uploading_pqc)\n",
        "    model = tf.keras.Model(inputs=[input_tensor], outputs=Q_values)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = generate_model_Qlearning(qubits, n_layers, n_actions, observables, False)\n",
        "model_target = generate_model_Qlearning(qubits, n_layers, n_actions, observables, True)\n",
        "\n",
        "model_target.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "eRJsyq9_vGaM"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def Q_learning_update(states, actions, rewards, next_states, done, model, gamma, n_actions):\n",
        "    states = tf.convert_to_tensor(states)\n",
        "    actions = tf.convert_to_tensor(actions)\n",
        "    rewards = tf.convert_to_tensor(rewards)\n",
        "    next_states = tf.convert_to_tensor(next_states)\n",
        "    done = tf.convert_to_tensor(done)\n",
        "\n",
        "    # Compute their target q_values and the masks on sampled actions\n",
        "    future_rewards = model_target([next_states])\n",
        "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1)\n",
        "                                                   * (1.0 - done))\n",
        "    masks = tf.one_hot(actions, n_actions)\n",
        "\n",
        "    # Train the model on the states and target Q-values\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        q_values = model([states])\n",
        "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
        "\n",
        "    # Backpropagation\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
        "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "lqTqpdDivKv0"
      },
      "outputs": [],
      "source": [
        "\n",
        "gamma = 1\n",
        "n_episodes = 5000\n",
        "\n",
        "# Define replay memory\n",
        "max_memory_length = 1000000 # Maximum replay length\n",
        "replay_memory = deque(maxlen=max_memory_length)\n",
        "\n",
        "epsilon = 0.7  # Epsilon greedy parameter\n",
        "epsilon_min = 0.3 # Minimum epsilon greedy parameter\n",
        "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter\n",
        "batch_size = 50\n",
        "steps_per_update = 5 # Train the model every x steps\n",
        "steps_per_target_update = 30 # Update the target model every x steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KxsovBtuvPel"
      },
      "outputs": [],
      "source": [
        "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
        "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
        "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "\n",
        "# Assign the model parameters to each optimizer\n",
        "w_in, w_var, w_out = 1, 0, 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfwZRw4UvlWN",
        "outputId": "551a97d1-a49e-4aee-8137-6cc2095f534a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10/5000, average last 10 rewards 930.0\n",
            "Episode 20/5000, average last 10 rewards -240.0\n",
            "Episode 30/5000, average last 10 rewards 486.0\n",
            "Episode 40/5000, average last 10 rewards 1608.8\n",
            "Episode 50/5000, average last 10 rewards -505.0\n",
            "Episode 60/5000, average last 10 rewards 264.0\n",
            "Episode 70/5000, average last 10 rewards 312.0\n",
            "Episode 80/5000, average last 10 rewards -504.0\n",
            "Episode 90/5000, average last 10 rewards 612.0\n",
            "Episode 100/5000, average last 10 rewards 234.8\n",
            "Episode 110/5000, average last 10 rewards 408.0\n",
            "Episode 120/5000, average last 10 rewards 792.0\n",
            "Episode 130/5000, average last 10 rewards 149.0\n",
            "Episode 140/5000, average last 10 rewards -181.6\n",
            "Episode 150/5000, average last 10 rewards 239.4\n",
            "Episode 160/5000, average last 10 rewards 576.4\n",
            "Episode 170/5000, average last 10 rewards -48.0\n",
            "Episode 180/5000, average last 10 rewards -12.0\n",
            "Episode 190/5000, average last 10 rewards -702.0\n",
            "Episode 200/5000, average last 10 rewards -325.2\n",
            "Episode 210/5000, average last 10 rewards -144.0\n",
            "Episode 220/5000, average last 10 rewards 29.2\n",
            "Episode 230/5000, average last 10 rewards 1668.0\n",
            "Episode 240/5000, average last 10 rewards 168.0\n",
            "Episode 250/5000, average last 10 rewards 149.4\n",
            "Episode 260/5000, average last 10 rewards -366.0\n",
            "Episode 270/5000, average last 10 rewards 29.0\n",
            "Episode 280/5000, average last 10 rewards -444.6\n",
            "Episode 290/5000, average last 10 rewards 36.0\n",
            "Episode 300/5000, average last 10 rewards -150.0\n",
            "Episode 310/5000, average last 10 rewards -240.6\n",
            "Episode 320/5000, average last 10 rewards 42.0\n",
            "Episode 330/5000, average last 10 rewards -385.2\n",
            "Episode 340/5000, average last 10 rewards 126.0\n",
            "Episode 350/5000, average last 10 rewards 330.0\n",
            "Episode 360/5000, average last 10 rewards 2147.0\n",
            "Episode 370/5000, average last 10 rewards -324.4\n",
            "Episode 380/5000, average last 10 rewards -667.0\n",
            "Episode 390/5000, average last 10 rewards 95.0\n",
            "Episode 400/5000, average last 10 rewards 126.0\n",
            "Episode 410/5000, average last 10 rewards -624.0\n",
            "Episode 420/5000, average last 10 rewards -1008.8\n",
            "Episode 430/5000, average last 10 rewards -72.8\n",
            "Episode 440/5000, average last 10 rewards 654.6\n",
            "Episode 450/5000, average last 10 rewards -703.0\n",
            "Episode 460/5000, average last 10 rewards -6.0\n",
            "Episode 470/5000, average last 10 rewards 18.0\n",
            "Episode 480/5000, average last 10 rewards 761.0\n",
            "Episode 490/5000, average last 10 rewards 228.0\n",
            "Episode 500/5000, average last 10 rewards -175.0\n",
            "Episode 510/5000, average last 10 rewards 47.2\n",
            "Episode 520/5000, average last 10 rewards 343.0\n",
            "Episode 530/5000, average last 10 rewards -588.0\n",
            "Episode 540/5000, average last 10 rewards -450.0\n",
            "Episode 550/5000, average last 10 rewards 312.0\n",
            "Episode 560/5000, average last 10 rewards 5.0\n",
            "Episode 570/5000, average last 10 rewards 114.0\n",
            "Episode 580/5000, average last 10 rewards -331.6\n",
            "Episode 590/5000, average last 10 rewards 57.6\n",
            "Episode 600/5000, average last 10 rewards 511.0\n",
            "Episode 610/5000, average last 10 rewards -587.0\n",
            "Episode 620/5000, average last 10 rewards -228.0\n",
            "Episode 630/5000, average last 10 rewards -37.4\n",
            "Episode 640/5000, average last 10 rewards 324.2\n",
            "Episode 650/5000, average last 10 rewards 78.0\n",
            "Episode 660/5000, average last 10 rewards -336.4\n",
            "Episode 670/5000, average last 10 rewards 72.0\n",
            "Episode 680/5000, average last 10 rewards 126.4\n",
            "Episode 690/5000, average last 10 rewards -900.0\n",
            "Episode 700/5000, average last 10 rewards -330.0\n",
            "Episode 710/5000, average last 10 rewards 647.0\n",
            "Episode 720/5000, average last 10 rewards -582.4\n",
            "Episode 730/5000, average last 10 rewards 312.2\n",
            "Episode 740/5000, average last 10 rewards 252.0\n",
            "Episode 750/5000, average last 10 rewards -42.0\n",
            "Episode 760/5000, average last 10 rewards -457.0\n",
            "Episode 770/5000, average last 10 rewards 348.0\n",
            "Episode 780/5000, average last 10 rewards -36.0\n",
            "Episode 790/5000, average last 10 rewards -787.2\n",
            "Episode 800/5000, average last 10 rewards 407.4\n",
            "Episode 810/5000, average last 10 rewards -121.4\n",
            "Episode 820/5000, average last 10 rewards 359.2\n",
            "Episode 830/5000, average last 10 rewards -331.2\n",
            "Episode 840/5000, average last 10 rewards 330.0\n",
            "Episode 850/5000, average last 10 rewards 258.0\n",
            "Episode 860/5000, average last 10 rewards -594.0\n",
            "Episode 870/5000, average last 10 rewards -30.0\n",
            "Episode 880/5000, average last 10 rewards -211.0\n",
            "Episode 890/5000, average last 10 rewards 17.2\n",
            "Episode 900/5000, average last 10 rewards -702.0\n",
            "Episode 910/5000, average last 10 rewards -1152.0\n",
            "Episode 920/5000, average last 10 rewards -529.0\n",
            "Episode 930/5000, average last 10 rewards -216.0\n",
            "Episode 940/5000, average last 10 rewards 156.0\n",
            "Episode 950/5000, average last 10 rewards -541.0\n",
            "Episode 960/5000, average last 10 rewards 582.0\n",
            "Episode 970/5000, average last 10 rewards -396.0\n",
            "Episode 980/5000, average last 10 rewards 21.0\n",
            "Episode 990/5000, average last 10 rewards 322.6\n",
            "Episode 1000/5000, average last 10 rewards -546.0\n",
            "Episode 1010/5000, average last 10 rewards 323.2\n",
            "Episode 1020/5000, average last 10 rewards 148.2\n",
            "Episode 1030/5000, average last 10 rewards -380.0\n",
            "Episode 1040/5000, average last 10 rewards -60.0\n",
            "Episode 1050/5000, average last 10 rewards 305.2\n",
            "Episode 1060/5000, average last 10 rewards -901.0\n",
            "Episode 1070/5000, average last 10 rewards 606.2\n",
            "Episode 1080/5000, average last 10 rewards -714.0\n",
            "Episode 1090/5000, average last 10 rewards -413.0\n",
            "Episode 1100/5000, average last 10 rewards 239.2\n",
            "Episode 1110/5000, average last 10 rewards -13.0\n",
            "Episode 1120/5000, average last 10 rewards -224.0\n",
            "Episode 1130/5000, average last 10 rewards -372.0\n",
            "Episode 1140/5000, average last 10 rewards -600.0\n",
            "Episode 1150/5000, average last 10 rewards 522.0\n",
            "Episode 1160/5000, average last 10 rewards -79.0\n",
            "Episode 1170/5000, average last 10 rewards -241.0\n",
            "Episode 1180/5000, average last 10 rewards -200.0\n",
            "Episode 1190/5000, average last 10 rewards -486.0\n",
            "Episode 1200/5000, average last 10 rewards -84.0\n",
            "Episode 1210/5000, average last 10 rewards -1734.0\n",
            "Episode 1220/5000, average last 10 rewards -419.8\n",
            "Episode 1230/5000, average last 10 rewards -246.0\n",
            "Episode 1240/5000, average last 10 rewards -168.0\n",
            "Episode 1250/5000, average last 10 rewards 300.2\n",
            "Episode 1260/5000, average last 10 rewards 347.2\n",
            "Episode 1270/5000, average last 10 rewards -174.0\n",
            "Episode 1280/5000, average last 10 rewards -288.0\n",
            "Episode 1290/5000, average last 10 rewards -798.0\n",
            "Episode 1300/5000, average last 10 rewards 456.0\n",
            "Episode 1310/5000, average last 10 rewards -372.0\n",
            "Episode 1320/5000, average last 10 rewards 192.0\n",
            "Episode 1330/5000, average last 10 rewards 462.0\n",
            "Episode 1340/5000, average last 10 rewards -43.0\n",
            "Episode 1350/5000, average last 10 rewards -420.0\n",
            "Episode 1360/5000, average last 10 rewards 71.2\n",
            "Episode 1370/5000, average last 10 rewards 240.0\n",
            "Episode 1380/5000, average last 10 rewards -264.8\n",
            "Episode 1390/5000, average last 10 rewards -158.0\n",
            "Episode 1400/5000, average last 10 rewards 322.0\n",
            "Episode 1410/5000, average last 10 rewards 297.8\n",
            "Episode 1420/5000, average last 10 rewards 305.4\n",
            "Episode 1430/5000, average last 10 rewards -722.0\n",
            "Episode 1440/5000, average last 10 rewards -620.2\n",
            "Episode 1450/5000, average last 10 rewards -90.0\n",
            "Episode 1460/5000, average last 10 rewards -486.0\n",
            "Episode 1470/5000, average last 10 rewards -475.2\n",
            "Episode 1480/5000, average last 10 rewards -384.6\n",
            "Episode 1490/5000, average last 10 rewards -695.0\n",
            "Episode 1500/5000, average last 10 rewards -36.4\n",
            "Episode 1510/5000, average last 10 rewards -696.0\n",
            "Episode 1520/5000, average last 10 rewards 413.2\n",
            "Episode 1530/5000, average last 10 rewards -48.0\n",
            "Episode 1540/5000, average last 10 rewards -445.0\n",
            "Episode 1550/5000, average last 10 rewards -6.0\n",
            "Episode 1560/5000, average last 10 rewards -690.6\n",
            "Episode 1570/5000, average last 10 rewards -126.0\n",
            "Episode 1580/5000, average last 10 rewards -12.0\n",
            "Episode 1590/5000, average last 10 rewards -205.0\n",
            "Episode 1600/5000, average last 10 rewards -223.0\n",
            "Episode 1610/5000, average last 10 rewards 670.2\n",
            "Episode 1620/5000, average last 10 rewards -396.0\n",
            "Episode 1630/5000, average last 10 rewards -474.0\n",
            "Episode 1640/5000, average last 10 rewards 282.0\n",
            "Episode 1650/5000, average last 10 rewards 174.0\n"
          ]
        }
      ],
      "source": [
        "import time \n",
        "import numpy as np\n",
        "from Environment import Environment \n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import deque, defaultdict\n",
        "import math\n",
        "import copy\n",
        "\n",
        "env = Environment(1000,100)\n",
        "env.CreateStates()\n",
        "episode_reward_history = []\n",
        "step_count = 0\n",
        "for episode in range(2000):\n",
        "    episode_reward = 0\n",
        "    env.reset_paramter()\n",
        "    state, _ = env.reset_state()\n",
        "    env.generate_channel_state_list_for_whole_sequence(state[1])\n",
        "    if state[1] == \"Ch1\":\n",
        "        state[1] = 1\n",
        "    else:\n",
        "        state[1] = 0\n",
        "    state = state.astype(np.float32)\n",
        "\n",
        "    while True:\n",
        "        # Interact with env\n",
        "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
        "\n",
        "        # Store interaction in the replay memory\n",
        "        replay_memory.append(interaction)\n",
        "\n",
        "        state = interaction['next_state']\n",
        "        episode_reward += interaction['reward']\n",
        "        step_count += 1\n",
        "\n",
        "        # Update model\n",
        "        if step_count % steps_per_update == 0 and len(replay_memory) > batch_size:\n",
        "            # Sample a batch of interactions and update Q_function\n",
        "            randnum = np.random.randint(0,math.floor(len(replay_memory)/batch_size))\n",
        "            crowler_memory = copy.deepcopy(replay_memory)\n",
        "            training_batch = np.array(list(crowler_memory)[randnum*batch_size:(randnum+1)*batch_size])          \n",
        "\n",
        "            Q_learning_update(np.asarray([x['state'] for x in training_batch]),\n",
        "                              np.asarray([x['action'] for x in training_batch]),\n",
        "                              np.asarray([x['reward'] for x in training_batch], dtype=np.float32),\n",
        "                              np.asarray([x['next_state'] for x in training_batch]),\n",
        "                              np.asarray([x['done'] for x in training_batch], dtype=np.float32),\n",
        "                              model, gamma, n_actions)\n",
        "\n",
        "        # Update target model\n",
        "        if step_count % steps_per_target_update == 0:\n",
        "            model_target.set_weights(model.get_weights())\n",
        "\n",
        "        # Check if the episode is finished\n",
        "        if interaction['done']:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    if episode % 10 == 0:\n",
        "      epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
        "      # print(epsilon)\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if (episode+1)%10 == 0:\n",
        "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
        "        print(\"Episode {}/{}, average last 10 rewards {}\".format(\n",
        "            episode+1, n_episodes, avg_rewards))\n",
        "        # if avg_rewards >= 500.0:\n",
        "        #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I4pM1SnPwvr0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def interact_env(state, model, epsilon, n_actions, env):\n",
        "  # Preprocess state\n",
        "  state_array = np.array(state) \n",
        "  state = tf.convert_to_tensor([state_array])\n",
        "\n",
        "  # Sample action\n",
        "  coin = np.random.random()\n",
        "  if coin > epsilon:\n",
        "      q_vals = model([state])\n",
        "      action = int(tf.argmax(q_vals[0]).numpy())\n",
        "  else:\n",
        "      action = np.random.choice(n_actions)\n",
        "\n",
        "\n",
        "  if env.state.Ra == 0 and env.state.U == 0:\n",
        "      action = 0\n",
        "  if env.state.U > 0:\n",
        "      action = 0\n",
        "  if env.sendbackaction == True:\n",
        "      action = 1\n",
        "  # Apply sampled action in the environment, receive reward and next state\n",
        "  next_state, reward, done = env.step(action)\n",
        "  if next_state[1] == \"Ch1\":\n",
        "      next_state[1] = 1\n",
        "  else:\n",
        "      next_state[1] = 0\n",
        "  next_state = next_state.astype(np.float32)\n",
        "  interaction = {'state': state_array, 'action': action, 'next_state': next_state.copy(),\n",
        "                'reward': reward, 'done':np.float32(done)}\n",
        "\n",
        "  return interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qwn7nfZAd4n"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "from tqdm import tqdm\n",
        "from collections import deque, defaultdict\n",
        "\n",
        "\n",
        "\n",
        "environment = Environment(1000,100)\n",
        "environment.CreateStates()\n",
        "episode = 0\n",
        "for i in (range(1000)):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state, _ = environment.reset_state()\n",
        "    if state[1] == \"Ch1\":\n",
        "        state[1] = 1\n",
        "    else:\n",
        "        state[1] = 0\n",
        "\n",
        "    state = state.astype(np.float32)\n",
        "    state_array = np.array(state) \n",
        "    state = tf.convert_to_tensor([state_array])\n",
        "    print(state)\n",
        "    q_vals = model([state])\n",
        "    action = int(tf.argmax(q_vals[0]).numpy())\n",
        "    print(action)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRSTdrjYD9ZIRBSIHB+A/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}