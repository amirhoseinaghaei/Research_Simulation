{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPspbVLPScMankf9IrbYg/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirhoseinaghaei/Research_Simulation/blob/main/Research_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KY0GC4fGZlks"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "import torch.optim as optim\n",
        "# from nn import LSTMLayer\n",
        "from torch.nn import MSELoss as MSELoss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class State(object):\n",
        "    def __init__(self , Name, Au , Ch, BT, Ra, U):\n",
        "        self.Name = Name\n",
        "        self.Au = Au\n",
        "        self.Ch = Ch\n",
        "        self.BT = BT\n",
        "        self.Ra = Ra\n",
        "        self.U = U\n"
      ],
      "metadata": {
        "id": "qsixrxN-Zn0R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DefiningWirelessChannels(object):\n",
        "    def __init__(self, NUM_Ch , Rate_List):\n",
        "        self.NUM_Ch = NUM_Ch\n",
        "        self.Rate_List = Rate_List\n",
        "        self.Rate_Dict = {}\n",
        "    def Create_RateDict(self):\n",
        "        for i in range(self.NUM_Ch):\n",
        "            self.Rate_Dict[f\"Ch{i+1}\"] = self.Rate_List[i]\n",
        "            "
      ],
      "metadata": {
        "id": "xpmkNOfmZrLg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "rnd_seed = 1\n",
        "torch.manual_seed(rnd_seed)\n",
        "np.random.seed(rnd_seed)\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.1\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "class Environment(object):\n",
        "    \n",
        "    def __init__(self, B_max, C_max , WirelessChannelClass = DefiningWirelessChannels(2,[5,20]), K = 3 , W = 30 , Fc = 5):\n",
        "        \n",
        "        self.inital_state = None\n",
        "        self.state = None\n",
        "        self.updated = True\n",
        "        self.update = None\n",
        "        self.deadline = 25\n",
        "        self.time = None\n",
        "        self.Wait = None\n",
        "        self.Fc = Fc\n",
        "        self.K = K\n",
        "        self.W = W\n",
        "        self.B_max = B_max\n",
        "        self.C_max = C_max\n",
        "        self.WirelessChannelClass = WirelessChannelClass\n",
        "        self.WirelessChannelClass.Create_RateDict()\n",
        "        self.ConfigureParameters()\n",
        "    \n",
        "    def ConfigureParameters(self):\n",
        "        self.BT_max = self.C_max + self.B_max\n",
        "        self.Au_min = (self.B_max/max(self.WirelessChannelClass.Rate_List)) + (self.C_max/self.Fc)\n",
        "        self.Au_max = self.W + self.W*self.K + (self.B_max/min(self.WirelessChannelClass.Rate_List)) + (self.C_max/self.Fc)\n",
        "        self.ADT_max = (self.B_max/min(self.WirelessChannelClass.Rate_List)) + (self.C_max/self.Fc)\n",
        "\n",
        "    def CreateStates(self):\n",
        "        D = self.ADT_max - self.Au_min\n",
        "        Deadline = 25\n",
        "        self.StateList = []\n",
        "        for i in range(int(self.Au_min),int(self.Au_max)+1):\n",
        "            if self.Au_min <= i <= self.W:\n",
        "                BT = 0\n",
        "                if self.Au_min <= i <= self.ADT_max + self.Au_min -1:\n",
        "                    U_max = i - self.Au_min\n",
        "                self.StateList.append(State(f\"({i}, Ch1, {BT}, {1}, {0})\", Au= i, Ch =\"Ch1\" , BT = BT, Ra = 1, U = 0))\n",
        "                self.StateList.append(State(f\"({i}, Ch2, {BT}, {1}, {0})\", Au= i, Ch =\"Ch2\" , BT = BT, Ra = 1, U = 0))\n",
        "                for j in range(0,int(U_max)+1):\n",
        "                    self.StateList.append(State(f\"({i}, Ch1, {BT}, {0}, {j})\", Au= i, Ch =\"Ch1\" , BT = BT, Ra = 0, U = j))\n",
        "                    self.StateList.append(State(f\"({i}, Ch2, {BT}, {0}, {j})\", Au= i, Ch =\"Ch2\" , BT = BT, Ra = 0, U = j))\n",
        "            if i > self.W and 0 < i%self.W <= self.Au_min:\n",
        "                first = self.BT_max - (i%self.W)*max(self.WirelessChannelClass.Rate_List)\n",
        "                \n",
        "                if first < self.C_max : \n",
        "                   \n",
        "                    BT = []\n",
        "                    Len = i%self.W- self.B_max/max(self.WirelessChannelClass.Rate_List)\n",
        "                    Len = int(Len)\n",
        "                    for j in range(Len+1):\n",
        "                        BT.append(self.C_max-(j)*self.Fc)\n",
        "                    BT.sort()\n",
        "                   \n",
        "                else:\n",
        "                    BT = [first]\n",
        "                last = self.BT_max - (i%self.W)*min(self.WirelessChannelClass.Rate_List)\n",
        "                crowler = first\n",
        "                while crowler != last:\n",
        "                    BT.append(int(crowler + D)) if crowler + D > 25 else None\n",
        "                    crowler = crowler + D\n",
        "                if self.W <= i < Deadline + self.Au_min :\n",
        "                    U_max = i - self.Au_min\n",
        "                    for bt in BT:\n",
        "                        self.StateList.append(State(f\"({i}, Ch1, {bt}, {1}, {0})\", Au= i, Ch =\"Ch1\" , BT = bt, Ra = 1, U = 0))\n",
        "                        self.StateList.append(State(f\"({i}, Ch2, {bt}, {1}, {0})\", Au= i, Ch =\"Ch2\" , BT = bt, Ra = 1, U = 0))\n",
        "                        for j in range(0,int(U_max)+1):\n",
        "                            self.StateList.append(State(f\"({i}, Ch1, {bt}, {0}, {j})\", Au= i, Ch =\"Ch1\" , BT = bt, Ra = 0, U = j))\n",
        "                            self.StateList.append(State(f\"({i}, Ch2, {bt}, {0}, {j})\", Au= i, Ch =\"Ch2\" , BT = bt, Ra = 0, U = j))\n",
        "                else: \n",
        "                    U_max = 25 \n",
        "                    for bt in BT:\n",
        "                        self.StateList.append(State(f\"({i}, Ch1, {bt}, {1}, {0})\", Au= i, Ch =\"Ch1\" , BT = bt, Ra = 1, U = 0))\n",
        "                        self.StateList.append(State(f\"({i}, Ch2, {bt}, {1}, {0})\", Au= i, Ch =\"Ch2\" , BT = bt, Ra = 1, U = 0))\n",
        "                        for j in range(0,int(U_max)+1):\n",
        "                            self.StateList.append(State(f\"({i}, Ch1, {bt}, {0}, {j})\", Au= i, Ch =\"Ch1\" , BT = bt, Ra = 0, U = j))\n",
        "                            self.StateList.append(State(f\"({i}, Ch2, {bt}, {0}, {j})\", Au= i, Ch =\"Ch2\" , BT = bt, Ra = 0, U = j))\n",
        "            if i> self.C_max and i%self.W > self.Au_min and i%self.W < self.B_max/min(self.WirelessChannelClass.Rate_List):\n",
        "                BT = [0,5,10,15,20,25]\n",
        "                first = self.BT_max - (i%self.W)*max(self.WirelessChannelClass.Rate_List)\n",
        "                if first <= 25 : \n",
        "                    BT = [0,5,10,15,20,25]\n",
        "                else:\n",
        "                    BT = [first]\n",
        "                last = self.BT_max - (i%self.W)*min(self.WirelessChannelClass.Rate_List)\n",
        "                crowler = first\n",
        "                while crowler != last:\n",
        "                    BT.append(int(crowler + D)) if crowler + D > 25 else None\n",
        "                    crowler = crowler + D           \n",
        "                U_max = 25 \n",
        "                for bt in BT:\n",
        "                    self.StateList.append(State(f\"({i}, Ch1, {bt}, {1}, {0})\", Au= i, Ch =\"Ch1\" , BT = bt, Ra = 1, U = 0))\n",
        "                    self.StateList.append(State(f\"({i}, Ch2, {bt}, {1}, {0})\", Au= i, Ch =\"Ch2\" , BT = bt, Ra = 1, U = 0))\n",
        "                    for j in range(0,int(U_max)+1):\n",
        "                        self.StateList.append(State(f\"({i}, Ch1, {bt}, {0}, {j})\", Au= i, Ch =\"Ch1\" , BT = bt, Ra = 0, U = j))\n",
        "                        self.StateList.append(State(f\"({i}, Ch2, {bt}, {0}, {j})\", Au= i, Ch =\"Ch2\" , BT = bt, Ra = 0, U = j))\n",
        "            if i> self.C_max and i%self.W > self.Au_min and i%self.W >= self.B_max/min(self.WirelessChannelClass.Rate_List):\n",
        "                BT = [0]\n",
        "                crowler = 0\n",
        "                if int(self.C_max - (i%self.W - ((self.B_max)/min(self.WirelessChannelClass.Rate_List)))*self.Fc) > 0:\n",
        "                    while crowler != int(self.C_max - (i%self.W - ((self.B_max)/min(self.WirelessChannelClass.Rate_List)))*self.Fc):\n",
        "                        BT.append(crowler + self.Fc)  \n",
        "                        crowler = crowler + self.Fc             \n",
        "                U_max = 25 \n",
        "                for bt in BT:\n",
        "                    self.StateList.append(State(f\"({i}, Ch1, {bt}, {1}, {0})\", Au= i, Ch =\"Ch1\" , BT = bt, Ra = 1, U = 0))\n",
        "                    self.StateList.append(State(f\"({i}, Ch2, {bt}, {1}, {0})\", Au= i, Ch =\"Ch2\" , BT = bt, Ra = 1, U = 0))\n",
        "                    for j in range(0,int(U_max)+1):\n",
        "                        self.StateList.append(State(f\"({i}, Ch1, {bt}, {0}, {j})\", Au= i, Ch =\"Ch1\" , BT = bt, Ra = 0, U = j))\n",
        "                        self.StateList.append(State(f\"({i}, Ch2, {bt}, {0}, {j})\", Au= i, Ch =\"Ch2\" , BT = bt, Ra = 0, U = j))\n",
        "\n",
        "        self.initial_State  = []\n",
        "        for i in self.StateList:\n",
        "            if i.Ra == 1:\n",
        "                self.initial_State.append(i)   \n",
        "\n",
        "    def reset_state(self):\n",
        "        self.state = random.choice(self.initial_State)\n",
        "        self.inital_state = copy.deepcopy(self.state)\n",
        "        return np.array([self.state.Au, self.state.Ch, self.state.BT, self.state.Ra, self.state.U])\n",
        "\n",
        "    def reset_paramter(self):\n",
        "        self.Wait = 0\n",
        "        self.time = 0\n",
        "        self.updated = False\n",
        "    def remained_BT_modification(self):\n",
        "        if self.state.Au % self.W < self.W-1 and self.state.BT == 0:\n",
        "            self.state.BT = 0\n",
        "        elif self.state.Au % self.W == self.W-1:\n",
        "            self.state.BT = self.BT_max\n",
        "        else:\n",
        "            if self.C_max < self.state.BT <= self.BT_max:\n",
        "                self.state.BT -= self.WirelessChannelClass.Rate_Dict[self.state.Ch]\n",
        "            else:\n",
        "                self.state.BT -= self.Fc\n",
        "    def wireless_channel_modification(self):\n",
        "        random_generated = random.uniform(0, 1)\n",
        "        if random_generated < 0.5:\n",
        "            self.state.Ch = \"Ch1\"\n",
        "        else:\n",
        "            self.state.Ch = \"Ch2\"\n",
        "    def request_pending_time_modification(self,action):\n",
        "        if action == 1:\n",
        "            self.state.U = 0\n",
        "        elif action == 0 and self.Sendback:\n",
        "            self.state.U = 0\n",
        "        else: \n",
        "            self.state.U += 1\n",
        "\n",
        "    def AoI_modification(self, action):\n",
        "        if self.Wait == True:  \n",
        "            if self.update == False:\n",
        "                self.state.Au += 1\n",
        "            elif self.update and action == 1:\n",
        "                self.state.Au += 1\n",
        "            elif self.update and action == 0:\n",
        "                self.state.Au = self.state.Au % self.W + 1 \n",
        "        else:\n",
        "            self.updated = True\n",
        "            if self.inital_state.BT == 0:\n",
        "                self.state.Au = (self.state.Au%self.W) + 1\n",
        "            else:\n",
        "                self.state.Au = self.W + (self.state.Au%self.W) + 1\n",
        "            self.Wait = True\n",
        "    def state_transition(self, action):\n",
        "            self.remained_BT_modification()\n",
        "            self.wireless_channel_modification()\n",
        "            self.request_pending_time_modification(action)\n",
        "            self.state.Ra = 0\n",
        "            self.AoI_modification(action)\n",
        "            self.state.Name = f\"({self.state.Au}, {self.state.Ch}, {self.state.BT}, {self.state.Ra}, {self.state.U})\" \n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        if self.time == 0:\n",
        "            self.Wait = action == 0\n",
        "            self.Sendback = action == 1\n",
        "        self.time += 1\n",
        "        done = self.time == self.deadline\n",
        "        self.update = self.state.BT == self.Fc\n",
        "        if self.update and self.updated == False and self.Wait:\n",
        "            print(\"updated\")\n",
        "            self.updated = True\n",
        "            if self.inital_state.Au >= self.W and self.inital_state.BT != 0:\n",
        "                    reward += self.W + (self.inital_state.Au)%(self.W)\n",
        "            else:\n",
        "                reward += (self.inital_state.Au)%(self.W)\n",
        "            reward -= (self.time + (self.state.Au)%(self.W) + 1)\n",
        "        \n",
        "        if done and self.Wait and self.updated == False:\n",
        "            reward += (self.inital_state.Au)%(self.W)\n",
        "            reward -= (self.time + ((self.state.Au + 1)%self.W + self.W))\n",
        "        self.state_transition(action)\n",
        "        \n",
        "        return np.array([self.state.Au, self.state.Ch, self.state.BT, 0, self.state.U ]) , reward , done\n"
      ],
      "metadata": {
        "id": "JsMw-8tFZsxg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.rnn import LSTM\n",
        "def to_one_hot(y, n_dims=None):\n",
        "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
        "    y_tensor = y.data if isinstance(y, Variable) else y\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
        "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
        "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
        "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot\n",
        "\n",
        "class RRLSTM(nn.Module):\n",
        "    def __init__(self, state_input_size, n_actions, buffer, n_units, lstm_lr, l2_regularization,\n",
        "                 return_scaling, lstm_batch_size=128, continuous_pred_factor=0.5):\n",
        "        super(RRLSTM, self).__init__()\n",
        "        self.buffer = buffer\n",
        "        self.return_scaling = return_scaling\n",
        "        self.lstm_batch_size = lstm_batch_size\n",
        "        self.continuous_pred_factor = continuous_pred_factor\n",
        "        self.n_actions = n_actions    \n",
        "        # Forget gate and output gate are deactivated as used in the Atari games, see Appendix S4.2.1\n",
        "        \n",
        "        self.lstm = LSTM(state_input_size + n_actions, n_units)\n",
        "        self.linear = nn.Linear(n_units, 1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lstm_lr, weight_decay=l2_regularization)\n",
        "        self.lstm_updates = 0\n",
        "    def forward(self, input):\n",
        "        print(input)\n",
        "        states, actions = input\n",
        "        # # Prepare input features\n",
        "        # repaired = states[:, :, 0:1]\n",
        "        # # print(f\"input: {input}\")\n",
        "        # # print(f\"repaired: {repaired}\")\n",
        "        # transport_cond = states[:, :, 1:3]\n",
        "        # # print(f\"transport_cond: {transport_cond}\")\n",
        "        # brands = to_one_hot(states[:, :, 3], 4)\n",
        "        # # print(f\"brands: {brands}\")\n",
        "        # time = states[:, :, 4:] / states.shape[1]\n",
        "        # # print(f\"brands: {states[:, :, 4:]}\")\n",
        "        # states = torch.cat([repaired, transport_cond, brands, time], 2)\n",
        "        actions = to_one_hot(actions, self.n_actions)\n",
        "        actions = torch.cat((actions, torch.zeros((actions.shape[0], 1, self.n_actions))), 1)\n",
        "        # print(f\"Hi{actions}\")\n",
        "        # print(f\"Hi{states}\")\n",
        "        print(f\"First{states.size()}\")\n",
        "        print(f\"Second{actions.size()}\")\n",
        "        input = torch.cat((states, actions), 2)\n",
        "        # Run the lstm\n",
        "        lstm_out = self.lstm.forward(input, return_all_seq_pos=True)\n",
        "        return self.linear(lstm_out[0])\n",
        "    def redistribute_reward(self, states, actions):\n",
        "        # Prepare LSTM inputs\n",
        "        states_var = Variable(torch.FloatTensor(states)).detach()\n",
        "        delta_states = torch.cat([states_var[:, 0:1, :], states_var[:, 1:, :] - states_var[:, :-1, :]], dim=1)\n",
        "        actions_var = Variable(torch.FloatTensor(actions)).detach()\n",
        "        # Calculate LSTM predictions\n",
        "        lstm_out = self.forward([delta_states, actions_var])\n",
        "        pred_g0 = torch.cat([torch.zeros_like(lstm_out[:, 0:1, :]), lstm_out], dim=1)[:, :-1, :]\n",
        "        # Difference of predictions of two consecutive timesteps.\n",
        "        redistributed_reward = pred_g0[:, 1:, 0] - pred_g0[:, :-1, 0]\n",
        "        new_reward = redistributed_reward * self.return_scaling\n",
        "        return new_reward\n",
        "\n",
        "    # Trains the LSTM until -on average- the main loss is below 0.25.\n",
        "    def train(self, episode):\n",
        "\n",
        "        i = 0\n",
        "        loss_average = 0.3\n",
        "        mse_loss = MSELoss(reduction=\"none\")\n",
        "        while loss_average > 0.15:\n",
        "            i += 1\n",
        "            self.lstm_updates += 1\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Get samples from the lesson buffer and prepare them.\n",
        "            states, actions, rewards, lenght = self.buffer.sample(self.lstm_batch_size)\n",
        "            lenght = lenght[:, 0]\n",
        "            states_var = Variable(torch.FloatTensor(states)).detach()\n",
        "            actions_var = Variable(torch.FloatTensor(actions)).detach()\n",
        "            rewards_var = Variable(torch.FloatTensor(rewards)).detach()\n",
        "\n",
        "            # Scale the returns as they might have high / low values.\n",
        "            returns = torch.sum(rewards_var, 1, keepdim=True) / self.return_scaling\n",
        "\n",
        "            # Calculate differences of states\n",
        "            delta_states = torch.cat([states_var[:, 0:1, :], states_var[:, 1:, :] - states_var[:, :-1, :]], dim=1)\n",
        "            print(f\"HIIIII{delta_states}\")\n",
        "            # Run the LSTM\n",
        "            lstm_out = self.forward([delta_states, actions_var])\n",
        "            predicted_G0 = lstm_out.squeeze()\n",
        "\n",
        "            # Loss calculations\n",
        "            all_timestep_loss = mse_loss(predicted_G0, returns.repeat(1, predicted_G0.size(1)))\n",
        "\n",
        "            # Loss at any position in the sequence\n",
        "            aux_loss = self.continuous_pred_factor * all_timestep_loss.mean()\n",
        "\n",
        "            # LSTM is mainly trained on getting the final prediction of g0 right.\n",
        "            main_loss = all_timestep_loss[range(self.lstm_batch_size), lenght[:] - 1].mean()\n",
        "\n",
        "            # LSTM update and loss tracking\n",
        "            lstm_loss = main_loss + aux_loss\n",
        "            lstm_loss.backward()\n",
        "            loss_np = lstm_loss.data.numpy()\n",
        "            main_loss_np = main_loss.data.numpy()\n",
        "            loss_average -= 0.01 * (loss_average - main_loss_np)\n",
        "            if main_loss_np > loss_average * 2:\n",
        "                loss_average = loss_np\n",
        "            self.optimizer.step()"
      ],
      "metadata": {
        "id": "vj05Z1q_aLES"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LessonBuffer:\n",
        "    def __init__(self, size, deadline, state_variables):\n",
        "        self.size = size \n",
        "        self.states_buffer = np.empty(shape=(size, deadline + 1, state_variables))\n",
        "        self.actions_buffer = np.empty(shape=(size, deadline))\n",
        "        self.rewards_buffer = np.empty(shape=(size, deadline))\n",
        "        self.lens_buffer = np.empty(shape=(size, 1), dtype=np.int32)\n",
        "        self.next_spot_to_add = 0\n",
        "        self.buffer_is_full = False\n",
        "        self.samples_since_last_training = 0       \n",
        "    def different_returns_encountered(self):\n",
        "        if self.buffer_is_full:\n",
        "            # print(np.unique(self.rewards_buffer[..., -1]).shape[0] > 1)\n",
        "            return np.unique(self.rewards_buffer[..., -1]).shape[0] > 1\n",
        "        else:\n",
        "            return np.unique(self.rewards_buffer[:self.next_spot_to_add, -1]).shape[0] > 1\n",
        "    def full_enough(self):\n",
        "        return self.buffer_is_full or self.next_spot_to_add > 256  \n",
        "    def add(self,states, actions, rewards):\n",
        "        traj_length = states.shape[0]\n",
        "        print(traj_length)\n",
        "        next_ind = self.next_spot_to_add\n",
        "        self.next_spot_to_add = self.next_spot_to_add + 1\n",
        "        if self.next_spot_to_add >= self.size:\n",
        "            self.buffer_is_full = True\n",
        "        self.next_spot_to_add = self.next_spot_to_add % self.size\n",
        "        # print(states)\n",
        "        # print(states.squeeze())\n",
        "        self.states_buffer[next_ind, :traj_length] = states.squeeze()\n",
        "        self.states_buffer[next_ind, traj_length:] = 0\n",
        "        self.actions_buffer[next_ind, :traj_length - 1] = actions\n",
        "        self.actions_buffer[next_ind, traj_length:] = 0\n",
        "        self.rewards_buffer[next_ind, :traj_length - 1] = rewards\n",
        "        self.rewards_buffer[next_ind, traj_length:] = 0\n",
        "        # self.lens_buffer[next_ind] = traj_length\n",
        "    def sample(self, batch_size):\n",
        "        self.samples_since_last_training = 0\n",
        "        if self.buffer_is_full: \n",
        "            indices = np.random.randint(0, self.size, batch_size)\n",
        "        else: \n",
        "            indices = np.random.randint(0, self.next_spot_to_add, batch_size)\n",
        "        return (self.states_buffer[indices, :, :], self.actions_buffer[indices, :],\n",
        "                self.rewards_buffer[indices, :], self.lens_buffer[indices, :])\n"
      ],
      "metadata": {
        "id": "fE1eQAQXaGRC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model = Environment(100,25)\n",
        "Model.CreateStates()\n",
        "Lesson_buffer = LessonBuffer(1000, 25, 5)\n",
        "episode = 0\n",
        "\n",
        "class Actor():\n",
        "    def __init__(self, environment, lr):\n",
        "        self.environment = environment\n",
        "        self.lr = lr \n",
        "        self.state = None\n",
        "    def act(self):\n",
        "        if self.environment.state.Ra == 0 and self.environment.state.U == 0:\n",
        "            action = 0\n",
        "        elif self.environment.state.Ra == 1 and self.environment.state.U == 0:\n",
        "            action = 1 if  random.uniform(0, 1) < 0.5 else 0\n",
        "        elif self.environment.state.Ra == 0 and self.environment.state.U == 24:\n",
        "            action = 1\n",
        "        elif self.environment.state.U > 0:\n",
        "            action = 0\n",
        "\n",
        "        self.state , reward , done = self.environment.step(action)\n",
        "        return self.state, action, reward, done\n",
        "actor = Actor(Model, 0.1)\n",
        "# actor.act()\n",
        "rudder_lstm = RRLSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer, n_units=n_lstm,\n",
        "                       lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                       lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "\n",
        "for i in range(1000):\n",
        "    episode += 1\n",
        "    actor.environment.reset_paramter()\n",
        "    state = actor.environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    done = False\n",
        "    while not done:\n",
        "        state, action, reward, done = actor.act()\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward)\n",
        "        if done: \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            rewards = np.array(rewards)\n",
        "            actions = np.array(actions)\n",
        "            Lesson_buffer.add(states = states, actions = actions, rewards = rewards)\n",
        "            # print(actions)\n",
        "            # print(states)\n",
        "            # print(rewards)\n",
        "            if  Lesson_buffer.full_enough():\n",
        "              # print(\"True\")\n",
        "            # If RUDDER is run, the LSTM is trained after each episode until its loss is below a threshold.\n",
        "            # Samples will be drawn from the lessons buffer.\n",
        "              if episode % 25 == 0:\n",
        "                print(\"READY FOR TRAIN\")\n",
        "                rudder_lstm.train(episode=episode)"
      ],
      "metadata": {
        "id": "88484GGTZ5sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KKXDqsCaP2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}