{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNHidfW8A0L0JiKPHj55uw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirhoseinaghaei/Research_Simulation/blob/main/Research_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fusermount -u drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Dxqj3aTbb0",
        "outputId": "21c0787f-ac36-46ed-eaf0-2b960368e31f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q4f9sUQQiyL",
        "outputId": "5fa2c2fe-7032-4928-c278-57eabed4157e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive/Research_Simmulation/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JJOOU4FRFDl",
        "outputId": "407e8da7-e353-47e2-e9b7-eb89f1339d9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Research_Simmulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "Lesson_buffer = LessonBuffer(1000, 25, 5)\n",
        "episode = 0\n",
        "rudder_lstm = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "\n",
        "rudder_lstm.load_state_dict(torch.load('rudder_lstm_a0.pt'))\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "# print(type(environment.StateList[0].Name))\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "visited_dict_0 = {}\n",
        "visited_dict_1 = {}\n",
        "for i in range(10000):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "\n",
        "    while not done:\n",
        "        if episode <= 24:\n",
        "          action = np.random.choice(2)\n",
        "        else:\n",
        "          if np.random.random() < 0.15:\n",
        "              action = np.random.choice(2)\n",
        "              if len(states) == 1 and action == 0:\n",
        "                if name not in visited_dict_0.keys(): \n",
        "                    visited_dict_0[name] = 1\n",
        "                else:\n",
        "                    visited_dict_0[name] += 1\n",
        "              if len(states) == 1 and action == 1:\n",
        "                if name not in visited_dict_1.keys(): \n",
        "                    visited_dict_1[name] = 1\n",
        "                else:\n",
        "                    visited_dict_1[name] += 1  \n",
        "      #    \n",
        "          else:\n",
        "            # if policy_updator.Quality[name,0] > policy_updator.Quality[name,1] else 1\n",
        "              action = 0 if policy_updator.Quality[name,0] > policy_updator.Quality[name,1] else 1\n",
        "              if len(states) == 1 and action == 0:\n",
        "                if name not in visited_dict_0.keys(): \n",
        "                    visited_dict_0[name] = 1\n",
        "                else:\n",
        "                    visited_dict_0[name] += 1\n",
        "              if len(states) == 1 and action == 1:\n",
        "                if name not in visited_dict_1.keys(): \n",
        "                    visited_dict_1[name] = 1\n",
        "                else:\n",
        "                    visited_dict_1[name] += 1\n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "    \n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            # Lesson_buffer.add(states = states, actions = actions, rewards = rewards)\n",
        "            # if  episode < 2000 and Lesson_buffer.full_enough() and Lesson_buffer.different_returns_encountered()  :\n",
        "            #         # print(\"different_returns_encountered\")        \n",
        "            #         # If RUDDER is run, the LSTM is trained after each episode until its loss is below a threshold.\n",
        "            #         # Samples will be drawn from the lessons buffer.\n",
        "            #         if episode % 25 == 0:\n",
        "\n",
        "            #             # print(\"True\")\n",
        "            #             print(episode)\n",
        "            #             rudder_lstm.train(episode=episode)\n",
        "            #         if episode >= 1800: \n",
        "            #             torch.save(rudder_lstm.state_dict(), 'rudder_lstm.pt')\n",
        "            #         # Then the LSTM is used to redistribute the reward.\n",
        "            print(rewards)\n",
        "            print(states)\n",
        "            print(actions)\n",
        "            \n",
        "            rewards = rudder_lstm.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            # policy_updator.Q_learning(actions= actions , states = states, rewards= rewards)\n",
        "            print(rewards)\n",
        "\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            print('{:15} {:15} {:15}'.format( keys[0] ,  keys[1], value))\n",
        "print(\"******************************************************************\")\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))\n",
        "print(\"******************************************************************\")\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > 0:  \n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))\n",
        "print(\"******************************************************************\")\n",
        "for value in visited_dict_0:\n",
        "        print('{:15} {:15} {:15}'.format(\"Wait\", value, visited_dict_0[value]))\n",
        "\n",
        "\n",
        "print(\"******************************************************************\")\n",
        "for value in visited_dict_1:\n",
        "        print('{:15} {:15} {:15}'.format(\"Send Back\", value, visited_dict_1[value]))\n"
      ],
      "metadata": {
        "id": "oC33fYwwVhrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "Lesson_buffer_a0 = LessonBuffer(1000, 25, 5)\n",
        "episode = 0\n",
        "rudder_lstm_a0 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a0, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "# rudder_lstm.load_state_dict(torch.load('rudder_lstm.pt'))\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "for i in range(2050):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "\n",
        "    while not done:\n",
        "        action = 0 \n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            Lesson_buffer_a0.add(states = states, actions = actions, rewards = rewards)\n",
        "            if  episode < 2000 and Lesson_buffer_a0.full_enough() and Lesson_buffer_a0.different_returns_encountered()  :\n",
        "                    if episode % 25 == 0:\n",
        "\n",
        "                        print(episode)\n",
        "                        rudder_lstm_a0.train(episode=episode)\n",
        "                    # if episode >= 1800: \n",
        "                    #     torch.save(rudder_lstm_a0.state_dict(), 'rudder_lstm_a0.pt')\n",
        "\n",
        "            rewards = rudder_lstm_a0.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            policy_updator.Q_learning(actions= actions , states = states, rewards= rewards)\n",
        "\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            print('{:15} {:15} {:15}'.format( keys[0] ,  keys[1], value))\n",
        "print(\"******************************************************************\")\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))\n"
      ],
      "metadata": {
        "id": "oI543CL8ZfeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "Lesson_buffer_a1 = LessonBuffer(1000, 25, 5)\n",
        "episode = 0\n",
        "\n",
        "rudder_lstm_a1 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a1, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "# rudder_lstm.load_state_dict(torch.load('rudder_lstm.pt'))\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "visited_dict_0 = {}\n",
        "for i in range(2050):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "\n",
        "    while not done:\n",
        "        action = 1 \n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            Lesson_buffer_a1.add(states = states, actions = actions, rewards = rewards)\n",
        "            if  episode < 2000 and Lesson_buffer_a1.full_enough() and Lesson_buffer_a1.different_returns_encountered()  :\n",
        "                    if episode % 25 == 0:\n",
        "\n",
        "                        # print(\"True\")\n",
        "                        print(episode)\n",
        "                        rudder_lstm_a1.train(episode=episode)\n",
        "                    if episode >= 1800: \n",
        "                        torch.save(rudder_lstm_a1.state_dict(), 'rudder_lstm_a1.pt')\n",
        "            rewards = rudder_lstm_a1.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            policy_updator.Q_learning(actions= actions , states = states, rewards= rewards)\n",
        "\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            print('{:15} {:15} {:15}'.format( keys[0] ,  keys[1], value))\n",
        "print(\"******************************************************************\")\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))\n"
      ],
      "metadata": {
        "id": "q3D3Vrcvbuo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "episode = 0\n",
        "\n",
        "\n",
        "rudder_lstm_a0.load_state_dict(torch.load('rudder_lstm_a0.pt'))\n",
        "rudder_lstm_a1.load_state_dict(torch.load('rudder_lstm_a1.pt'))\n",
        "\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "for i in range(5000):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "\n",
        "    while not done:\n",
        "        if episode <= 24:\n",
        "          action = np.random.choice(2)\n",
        "        else:\n",
        "          if np.random.random() < 0.15:\n",
        "              action = np.random.choice(2) \n",
        "          else:\n",
        "              action = 0 if policy_updator.Quality[name,0] > policy_updator.Quality[name,1] else 1\n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "    \n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            if actions[0] == 0: \n",
        "              rewards = rudder_lstm_a0.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            if actions[0] == 1: \n",
        "              rewards = rudder_lstm_a1.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            \n",
        "            policy_updator.Q_learning(actions= actions , states = states, rewards= rewards)\n",
        "\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            print('{:15} {:15} {:15}'.format( keys[0] ,  keys[1], value))\n",
        "print(\"******************************************************************\")\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))"
      ],
      "metadata": {
        "id": "alRHIJG-dAQx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}