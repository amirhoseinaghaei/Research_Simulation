{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDJNzGdrm+IiOrscuamFAE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirhoseinaghaei/Research_Simulation/blob/main/Research_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fusermount -u drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Dxqj3aTbb0",
        "outputId": "3348cf28-28a5-464a-8c64-077152113cf1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q4f9sUQQiyL",
        "outputId": "b9130d49-65ff-42db-f51a-0f4a2041728f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive/Research_Simmulation/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JJOOU4FRFDl",
        "outputId": "5a5f3b92-38b3-410e-e2be-5dba449be29e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Research_Simmulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "Lesson_buffer_a0 = LessonBuffer(1000, 25, 5)\n",
        "episode = 0\n",
        "rudder_lstm_a0 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a0, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "rudder_lstm_a0.load_state_dict(torch.load('rudder_lstm_general09.pt'))\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "for i in range(2000):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state, _ = environment.reset_state()\n",
        "    environment.generate_channel_state_list_for_whole_sequence(state[1])\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "# if policy_updator.Quality[name,0] > policy_updator.Quality[name,1] else 1\n",
        "    while not done:\n",
        "        action = 0 \n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[-1] = rewards[-1]/10\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            # Lesson_buffer_a0.add(states = states, actions = actions, rewards = rewards)\n",
        "            # if  episode < 5000 and Lesson_buffer_a0.full_enough() and Lesson_buffer_a0.different_returns_encountered()  :\n",
        "            #         if episode % 25 == 0:\n",
        "\n",
        "            #             print(episode)\n",
        "            #             rudder_lstm_a0.train(episode=episode)\n",
        "            #         if episode >= 1800: \n",
        "            #             torch.save(rudder_lstm_a0.state_dict(), 'rudder_lstm_120.pt')\n",
        "\n",
        "            rewards = rudder_lstm_a0.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            policy_updator.Q_learning(actions= actions , states = states, rewards= rewards)\n",
        "\n",
        "Optimal_Policy_Dict = {}\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "              Optimal_Policy_Dict[keys[0]] = \"wait\"\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              Optimal_Policy_Dict[keys[0]] = \"send\"\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))\n",
        "print(Optimal_Policy_Dict)"
      ],
      "metadata": {
        "id": "oI543CL8ZfeZ",
        "outputId": "90574e96-9a9b-4213-dae8-c3c73d9460df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9, 0.09999999999999998], [1.0, 0.0]]\n",
            "[[0.9, 0.09999999999999998], [1.0, 0.0]]\n",
            "Amir\n",
            "(31, Ch1, 120, 1, 0) Wait            2.6542325019836426\n",
            "(31, Ch1, 120, 1, 0) Wait            2.6542325019836426\n",
            "(31, Ch2, 120, 1, 0) Wait            24.466270446777344\n",
            "(31, Ch2, 120, 1, 0) Wait            24.466270446777344\n",
            "(32, Ch1, 100, 1, 0) Wait            12.402787208557129\n",
            "(32, Ch1, 100, 1, 0) Wait            12.402787208557129\n",
            "(32, Ch2, 100, 1, 0) Wait            50.470428466796875\n",
            "(32, Ch2, 100, 1, 0) Wait            50.470428466796875\n",
            "(35, Ch1, 70, 1, 0) Wait            33.93714904785156\n",
            "(35, Ch1, 70, 1, 0) Wait            33.93714904785156\n",
            "(35, Ch2, 70, 1, 0) Wait            47.22541809082031\n",
            "(35, Ch2, 70, 1, 0) Wait            47.22541809082031\n",
            "(35, Ch1, 100, 1, 0) Wait            12.794124603271484\n",
            "(35, Ch1, 100, 1, 0) Wait            12.794124603271484\n",
            "(35, Ch2, 100, 1, 0) Wait            45.769893646240234\n",
            "(35, Ch2, 100, 1, 0) Wait            45.769893646240234\n",
            "(36, Ch1, 50, 1, 0) Wait            40.60774230957031\n",
            "(36, Ch1, 50, 1, 0) Wait            40.60774230957031\n",
            "(36, Ch2, 50, 1, 0) Wait            78.22636413574219\n",
            "(36, Ch2, 50, 1, 0) Wait            78.22636413574219\n",
            "(38, Ch1, 70, 1, 0) Wait            35.152748107910156\n",
            "(38, Ch1, 70, 1, 0) Wait            35.152748107910156\n",
            "(38, Ch2, 70, 1, 0) Wait            50.029632568359375\n",
            "(38, Ch2, 70, 1, 0) Wait            50.029632568359375\n",
            "(39, Ch1, 50, 1, 0) Wait            44.01112365722656\n",
            "(39, Ch1, 50, 1, 0) Wait            44.01112365722656\n",
            "(39, Ch2, 50, 1, 0) Wait            66.86278533935547\n",
            "(39, Ch2, 50, 1, 0) Wait            66.86278533935547\n",
            "(41, Ch1, 70, 1, 0) Wait            36.2730712890625\n",
            "(41, Ch1, 70, 1, 0) Wait            36.2730712890625\n",
            "(41, Ch2, 70, 1, 0) Wait            51.65674591064453\n",
            "(41, Ch2, 70, 1, 0) Wait            51.65674591064453\n",
            "(42, Ch1, 50, 1, 0) Wait            47.251502990722656\n",
            "(42, Ch1, 50, 1, 0) Wait            47.251502990722656\n",
            "(42, Ch2, 50, 1, 0) Wait            65.02198791503906\n",
            "(42, Ch2, 50, 1, 0) Wait            65.02198791503906\n",
            "(45, Ch1, 50, 1, 0) Wait            44.562416076660156\n",
            "(45, Ch1, 50, 1, 0) Wait            44.562416076660156\n",
            "(45, Ch2, 50, 1, 0) Wait            70.10306549072266\n",
            "(45, Ch2, 50, 1, 0) Wait            70.10306549072266\n",
            "(61, Ch1, 120, 1, 0) Send Back                     0\n",
            "(61, Ch1, 120, 1, 0) Send Back                     0\n",
            "(61, Ch2, 120, 1, 0) Send Back                     0\n",
            "(61, Ch2, 120, 1, 0) Send Back                     0\n",
            "(62, Ch1, 100, 1, 0) Send Back                     0\n",
            "(62, Ch1, 100, 1, 0) Send Back                     0\n",
            "(62, Ch2, 100, 1, 0) Send Back                     0\n",
            "(62, Ch2, 100, 1, 0) Send Back                     0\n",
            "(65, Ch1, 70, 1, 0) Send Back                     0\n",
            "(65, Ch1, 70, 1, 0) Send Back                     0\n",
            "(65, Ch2, 70, 1, 0) Wait            15.53683853149414\n",
            "(65, Ch2, 70, 1, 0) Wait            15.53683853149414\n",
            "(65, Ch1, 100, 1, 0) Send Back                     0\n",
            "(65, Ch1, 100, 1, 0) Send Back                     0\n",
            "(65, Ch2, 100, 1, 0) Send Back                     0\n",
            "(65, Ch2, 100, 1, 0) Send Back                     0\n",
            "(66, Ch1, 50, 1, 0) Wait            12.583412170410156\n",
            "(66, Ch1, 50, 1, 0) Wait            12.583412170410156\n",
            "(66, Ch2, 50, 1, 0) Wait            45.45362091064453\n",
            "(66, Ch2, 50, 1, 0) Wait            45.45362091064453\n",
            "(68, Ch1, 70, 1, 0) Send Back                     0\n",
            "(68, Ch1, 70, 1, 0) Send Back                     0\n",
            "(68, Ch2, 70, 1, 0) Wait            4.534465789794922\n",
            "(68, Ch2, 70, 1, 0) Wait            4.534465789794922\n",
            "(69, Ch1, 50, 1, 0) Wait            10.22791576385498\n",
            "(69, Ch1, 50, 1, 0) Wait            10.22791576385498\n",
            "(69, Ch2, 50, 1, 0) Wait            37.48789978027344\n",
            "(69, Ch2, 50, 1, 0) Wait            37.48789978027344\n",
            "(71, Ch1, 70, 1, 0) Send Back                     0\n",
            "(71, Ch1, 70, 1, 0) Send Back                     0\n",
            "(71, Ch2, 70, 1, 0) Wait            0.017395079135894775\n",
            "(71, Ch2, 70, 1, 0) Wait            0.017395079135894775\n",
            "(72, Ch1, 50, 1, 0) Wait            3.5625858306884766\n",
            "(72, Ch1, 50, 1, 0) Wait            3.5625858306884766\n",
            "(72, Ch2, 50, 1, 0) Wait            29.588706970214844\n",
            "(72, Ch2, 50, 1, 0) Wait            29.588706970214844\n",
            "(75, Ch1, 50, 1, 0) Wait            4.113073348999023\n",
            "(75, Ch1, 50, 1, 0) Wait            4.113073348999023\n",
            "(75, Ch2, 50, 1, 0) Wait            22.659942626953125\n",
            "(75, Ch2, 50, 1, 0) Wait            22.659942626953125\n",
            "(91, Ch1, 120, 1, 0) Send Back                     0\n",
            "(91, Ch1, 120, 1, 0) Send Back                     0\n",
            "(91, Ch2, 120, 1, 0) Send Back                     0\n",
            "(91, Ch2, 120, 1, 0) Send Back                     0\n",
            "(92, Ch1, 100, 1, 0) Send Back                     0\n",
            "(92, Ch1, 100, 1, 0) Send Back                     0\n",
            "(92, Ch2, 100, 1, 0) Send Back                     0\n",
            "(92, Ch2, 100, 1, 0) Send Back                     0\n",
            "(95, Ch1, 70, 1, 0) Send Back                     0\n",
            "(95, Ch1, 70, 1, 0) Send Back                     0\n",
            "(95, Ch2, 70, 1, 0) Send Back                     0\n",
            "(95, Ch2, 70, 1, 0) Send Back                     0\n",
            "(95, Ch1, 100, 1, 0) Send Back                     0\n",
            "(95, Ch1, 100, 1, 0) Send Back                     0\n",
            "(95, Ch2, 100, 1, 0) Send Back                     0\n",
            "(95, Ch2, 100, 1, 0) Send Back                     0\n",
            "(96, Ch1, 50, 1, 0) Send Back                     0\n",
            "(96, Ch1, 50, 1, 0) Send Back                     0\n",
            "(96, Ch2, 50, 1, 0) Wait            32.488128662109375\n",
            "(96, Ch2, 50, 1, 0) Wait            32.488128662109375\n",
            "(98, Ch1, 70, 1, 0) Send Back                     0\n",
            "(98, Ch1, 70, 1, 0) Send Back                     0\n",
            "(98, Ch2, 70, 1, 0) Send Back                     0\n",
            "(98, Ch2, 70, 1, 0) Send Back                     0\n",
            "(99, Ch1, 50, 1, 0) Send Back                     0\n",
            "(99, Ch1, 50, 1, 0) Send Back                     0\n",
            "(99, Ch2, 50, 1, 0) Wait            24.274921417236328\n",
            "(99, Ch2, 50, 1, 0) Wait            24.274921417236328\n",
            "(101, Ch1, 70, 1, 0) Send Back                     0\n",
            "(101, Ch1, 70, 1, 0) Send Back                     0\n",
            "(101, Ch2, 70, 1, 0) Send Back                     0\n",
            "(101, Ch2, 70, 1, 0) Send Back                     0\n",
            "(102, Ch1, 50, 1, 0) Send Back                     0\n",
            "(102, Ch1, 50, 1, 0) Send Back                     0\n",
            "(102, Ch2, 50, 1, 0) Wait            17.022632598876953\n",
            "(102, Ch2, 50, 1, 0) Wait            17.022632598876953\n",
            "(105, Ch1, 50, 1, 0) Send Back                     0\n",
            "(105, Ch1, 50, 1, 0) Send Back                     0\n",
            "(105, Ch2, 50, 1, 0) Wait            11.114899635314941\n",
            "(105, Ch2, 50, 1, 0) Wait            11.114899635314941\n",
            "(121, Ch1, 120, 1, 0) Send Back                     0\n",
            "(121, Ch1, 120, 1, 0) Send Back                     0\n",
            "(121, Ch2, 120, 1, 0) Send Back                     0\n",
            "(121, Ch2, 120, 1, 0) Send Back                     0\n",
            "(122, Ch1, 100, 1, 0) Send Back                     0\n",
            "(122, Ch1, 100, 1, 0) Send Back                     0\n",
            "(122, Ch2, 100, 1, 0) Send Back                     0\n",
            "(122, Ch2, 100, 1, 0) Send Back                     0\n",
            "(125, Ch1, 70, 1, 0) Send Back                     0\n",
            "(125, Ch1, 70, 1, 0) Send Back                     0\n",
            "(125, Ch2, 70, 1, 0) Send Back                     0\n",
            "(125, Ch2, 70, 1, 0) Send Back                     0\n",
            "(125, Ch1, 100, 1, 0) Send Back                     0\n",
            "(125, Ch1, 100, 1, 0) Send Back                     0\n",
            "(125, Ch2, 100, 1, 0) Send Back                     0\n",
            "(125, Ch2, 100, 1, 0) Send Back                     0\n",
            "(126, Ch1, 50, 1, 0) Send Back                     0\n",
            "(126, Ch1, 50, 1, 0) Send Back                     0\n",
            "(126, Ch2, 50, 1, 0) Wait            10.35965347290039\n",
            "(126, Ch2, 50, 1, 0) Wait            10.35965347290039\n",
            "(128, Ch1, 70, 1, 0) Send Back                     0\n",
            "(128, Ch1, 70, 1, 0) Send Back                     0\n",
            "(128, Ch2, 70, 1, 0) Send Back                     0\n",
            "(128, Ch2, 70, 1, 0) Send Back                     0\n",
            "(129, Ch1, 50, 1, 0) Send Back                     0\n",
            "(129, Ch1, 50, 1, 0) Send Back                     0\n",
            "(129, Ch2, 50, 1, 0) Send Back                     0\n",
            "(129, Ch2, 50, 1, 0) Send Back                     0\n",
            "(131, Ch1, 70, 1, 0) Send Back                     0\n",
            "(131, Ch1, 70, 1, 0) Send Back                     0\n",
            "(131, Ch2, 70, 1, 0) Send Back                     0\n",
            "(131, Ch2, 70, 1, 0) Send Back                     0\n",
            "(132, Ch1, 50, 1, 0) Send Back                     0\n",
            "(132, Ch1, 50, 1, 0) Send Back                     0\n",
            "(132, Ch2, 50, 1, 0) Send Back                     0\n",
            "(132, Ch2, 50, 1, 0) Send Back                     0\n",
            "(135, Ch1, 50, 1, 0) Send Back                     0\n",
            "(135, Ch1, 50, 1, 0) Send Back                     0\n",
            "(135, Ch2, 50, 1, 0) Send Back                     0\n",
            "(135, Ch2, 50, 1, 0) Send Back                     0\n",
            "{'(31, Ch1, 120, 1, 0)': 'wait', '(31, Ch2, 120, 1, 0)': 'wait', '(32, Ch1, 100, 1, 0)': 'wait', '(32, Ch2, 100, 1, 0)': 'wait', '(35, Ch1, 70, 1, 0)': 'wait', '(35, Ch2, 70, 1, 0)': 'wait', '(35, Ch1, 100, 1, 0)': 'wait', '(35, Ch2, 100, 1, 0)': 'wait', '(36, Ch1, 50, 1, 0)': 'wait', '(36, Ch2, 50, 1, 0)': 'wait', '(38, Ch1, 70, 1, 0)': 'wait', '(38, Ch2, 70, 1, 0)': 'wait', '(39, Ch1, 50, 1, 0)': 'wait', '(39, Ch2, 50, 1, 0)': 'wait', '(41, Ch1, 70, 1, 0)': 'wait', '(41, Ch2, 70, 1, 0)': 'wait', '(42, Ch1, 50, 1, 0)': 'wait', '(42, Ch2, 50, 1, 0)': 'wait', '(45, Ch1, 50, 1, 0)': 'wait', '(45, Ch2, 50, 1, 0)': 'wait', '(61, Ch1, 120, 1, 0)': 'send', '(61, Ch2, 120, 1, 0)': 'send', '(62, Ch1, 100, 1, 0)': 'send', '(62, Ch2, 100, 1, 0)': 'send', '(65, Ch1, 70, 1, 0)': 'send', '(65, Ch2, 70, 1, 0)': 'wait', '(65, Ch1, 100, 1, 0)': 'send', '(65, Ch2, 100, 1, 0)': 'send', '(66, Ch1, 50, 1, 0)': 'wait', '(66, Ch2, 50, 1, 0)': 'wait', '(68, Ch1, 70, 1, 0)': 'send', '(68, Ch2, 70, 1, 0)': 'wait', '(69, Ch1, 50, 1, 0)': 'wait', '(69, Ch2, 50, 1, 0)': 'wait', '(71, Ch1, 70, 1, 0)': 'send', '(71, Ch2, 70, 1, 0)': 'wait', '(72, Ch1, 50, 1, 0)': 'wait', '(72, Ch2, 50, 1, 0)': 'wait', '(75, Ch1, 50, 1, 0)': 'wait', '(75, Ch2, 50, 1, 0)': 'wait', '(91, Ch1, 120, 1, 0)': 'send', '(91, Ch2, 120, 1, 0)': 'send', '(92, Ch1, 100, 1, 0)': 'send', '(92, Ch2, 100, 1, 0)': 'send', '(95, Ch1, 70, 1, 0)': 'send', '(95, Ch2, 70, 1, 0)': 'send', '(95, Ch1, 100, 1, 0)': 'send', '(95, Ch2, 100, 1, 0)': 'send', '(96, Ch1, 50, 1, 0)': 'send', '(96, Ch2, 50, 1, 0)': 'wait', '(98, Ch1, 70, 1, 0)': 'send', '(98, Ch2, 70, 1, 0)': 'send', '(99, Ch1, 50, 1, 0)': 'send', '(99, Ch2, 50, 1, 0)': 'wait', '(101, Ch1, 70, 1, 0)': 'send', '(101, Ch2, 70, 1, 0)': 'send', '(102, Ch1, 50, 1, 0)': 'send', '(102, Ch2, 50, 1, 0)': 'wait', '(105, Ch1, 50, 1, 0)': 'send', '(105, Ch2, 50, 1, 0)': 'wait', '(121, Ch1, 120, 1, 0)': 'send', '(121, Ch2, 120, 1, 0)': 'send', '(122, Ch1, 100, 1, 0)': 'send', '(122, Ch2, 100, 1, 0)': 'send', '(125, Ch1, 70, 1, 0)': 'send', '(125, Ch2, 70, 1, 0)': 'send', '(125, Ch1, 100, 1, 0)': 'send', '(125, Ch2, 100, 1, 0)': 'send', '(126, Ch1, 50, 1, 0)': 'send', '(126, Ch2, 50, 1, 0)': 'wait', '(128, Ch1, 70, 1, 0)': 'send', '(128, Ch2, 70, 1, 0)': 'send', '(129, Ch1, 50, 1, 0)': 'send', '(129, Ch2, 50, 1, 0)': 'send', '(131, Ch1, 70, 1, 0)': 'send', '(131, Ch2, 70, 1, 0)': 'send', '(132, Ch1, 50, 1, 0)': 'send', '(132, Ch2, 50, 1, 0)': 'send', '(135, Ch1, 50, 1, 0)': 'send', '(135, Ch2, 50, 1, 0)': 'send'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "Lesson_buffer_a1 = LessonBuffer(1000, 25, 5)\n",
        "episode = 0\n",
        "\n",
        "rudder_lstm_a1 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a1, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "# rudder_lstm.load_state_dict(torch.load('rudder_lstm.pt'))\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "visited_dict_0 = {}\n",
        "for i in range(5000):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "\n",
        "    while not done:\n",
        "        action = 1 \n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            Lesson_buffer_a1.add(states = states, actions = actions, rewards = rewards)\n",
        "            if  episode < 2000 and Lesson_buffer_a1.full_enough() and Lesson_buffer_a1.different_returns_encountered()  :\n",
        "                    if episode % 25 == 0:\n",
        "\n",
        "                        # print(\"True\")\n",
        "                        print(episode)\n",
        "                        rudder_lstm_a1.train(episode=episode)\n",
        "                    if episode >= 1800: \n",
        "                        torch.save(rudder_lstm_a1.state_dict(), 'rudder_lstm_send_100.pt')\n",
        "            rewards = rudder_lstm_a1.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            policy_updator.Q_learning(actions= actions , states = states, rewards= rewards)\n",
        "\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            print('{:15} {:15} {:15}'.format( keys[0] ,  keys[1], value))\n",
        "print(\"******************************************************************\")\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))\n"
      ],
      "metadata": {
        "id": "q3D3Vrcvbuo0",
        "outputId": "d3d08adc-1f7f-4f43-8500-bee74500b2eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.75, 0.25], [0.85, 0.15000000000000002]]\n",
            "Amir\n",
            "275\n",
            "300\n",
            "325\n",
            "350\n",
            "375\n",
            "400\n",
            "425\n",
            "450\n",
            "475\n",
            "500\n",
            "525\n",
            "550\n",
            "575\n",
            "600\n",
            "625\n",
            "650\n",
            "675\n",
            "700\n",
            "725\n",
            "750\n",
            "775\n",
            "800\n",
            "825\n",
            "850\n",
            "875\n",
            "900\n",
            "925\n",
            "950\n",
            "975\n",
            "1000\n",
            "1025\n",
            "1050\n",
            "1075\n",
            "1100\n",
            "1125\n",
            "1150\n",
            "1175\n",
            "1200\n",
            "1225\n",
            "1250\n",
            "1275\n",
            "1300\n",
            "1325\n",
            "1350\n",
            "1375\n",
            "1400\n",
            "1425\n",
            "1450\n",
            "1475\n",
            "1500\n",
            "1525\n",
            "1550\n",
            "1575\n",
            "1600\n",
            "1625\n",
            "1650\n",
            "1675\n",
            "1700\n",
            "1725\n",
            "1750\n",
            "1775\n",
            "1800\n",
            "1825\n",
            "1850\n",
            "1875\n",
            "1900\n",
            "1925\n",
            "1950\n",
            "1975\n",
            "(31, Ch1, 120, 1, 0)               0               0\n",
            "(31, Ch1, 120, 1, 0)               1 13.735158920288086\n",
            "(31, Ch2, 120, 1, 0)               0               0\n",
            "(31, Ch2, 120, 1, 0)               1 12.685922622680664\n",
            "(36, Ch1, 50, 1, 0)               0               0\n",
            "(36, Ch1, 50, 1, 0)               1 5.22218132019043\n",
            "(36, Ch2, 50, 1, 0)               0               0\n",
            "(36, Ch2, 50, 1, 0)               1 0.797095775604248\n",
            "(39, Ch1, 50, 1, 0)               0               0\n",
            "(39, Ch1, 50, 1, 0)               1 7.4098968505859375\n",
            "(39, Ch2, 50, 1, 0)               0               0\n",
            "(39, Ch2, 50, 1, 0)               1 7.55939245223999\n",
            "(42, Ch1, 50, 1, 0)               0               0\n",
            "(42, Ch1, 50, 1, 0)               1 6.192809104919434\n",
            "(42, Ch2, 50, 1, 0)               0               0\n",
            "(42, Ch2, 50, 1, 0)               1 8.644883155822754\n",
            "(45, Ch1, 50, 1, 0)               0               0\n",
            "(45, Ch1, 50, 1, 0)               1 8.134713172912598\n",
            "(45, Ch2, 50, 1, 0)               0               0\n",
            "(45, Ch2, 50, 1, 0)               1 8.475969314575195\n",
            "(61, Ch1, 120, 1, 0)               0               0\n",
            "(61, Ch1, 120, 1, 0)               1 14.871761322021484\n",
            "(61, Ch2, 120, 1, 0)               0               0\n",
            "(61, Ch2, 120, 1, 0)               1 12.297138214111328\n",
            "(66, Ch1, 50, 1, 0)               0               0\n",
            "(66, Ch1, 50, 1, 0)               1 -0.6114863157272339\n",
            "(66, Ch2, 50, 1, 0)               0               0\n",
            "(66, Ch2, 50, 1, 0)               1 -2.5972328186035156\n",
            "(69, Ch1, 50, 1, 0)               0               0\n",
            "(69, Ch1, 50, 1, 0)               1 3.7653908729553223\n",
            "(69, Ch2, 50, 1, 0)               0               0\n",
            "(69, Ch2, 50, 1, 0)               1 1.2179077863693237\n",
            "(72, Ch1, 50, 1, 0)               0               0\n",
            "(72, Ch1, 50, 1, 0)               1 1.8534140586853027\n",
            "(72, Ch2, 50, 1, 0)               0               0\n",
            "(72, Ch2, 50, 1, 0)               1 2.354914665222168\n",
            "(75, Ch1, 50, 1, 0)               0               0\n",
            "(75, Ch1, 50, 1, 0)               1 4.470059394836426\n",
            "(75, Ch2, 50, 1, 0)               0               0\n",
            "(75, Ch2, 50, 1, 0)               1 2.8846640586853027\n",
            "(91, Ch1, 120, 1, 0)               0               0\n",
            "(91, Ch1, 120, 1, 0)               1 10.2926607131958\n",
            "(91, Ch2, 120, 1, 0)               0               0\n",
            "(91, Ch2, 120, 1, 0)               1 8.920186996459961\n",
            "(96, Ch1, 50, 1, 0)               0               0\n",
            "(96, Ch1, 50, 1, 0)               1 -0.6873733997344971\n",
            "(96, Ch2, 50, 1, 0)               0               0\n",
            "(96, Ch2, 50, 1, 0)               1 -8.911710739135742\n",
            "(99, Ch1, 50, 1, 0)               0               0\n",
            "(99, Ch1, 50, 1, 0)               1 2.88713002204895\n",
            "(99, Ch2, 50, 1, 0)               0               0\n",
            "(99, Ch2, 50, 1, 0)               1 1.8777236938476562\n",
            "(102, Ch1, 50, 1, 0)               0               0\n",
            "(102, Ch1, 50, 1, 0)               1 1.5050315856933594\n",
            "(102, Ch2, 50, 1, 0)               0               0\n",
            "(102, Ch2, 50, 1, 0)               1 0.3621219992637634\n",
            "(105, Ch1, 50, 1, 0)               0               0\n",
            "(105, Ch1, 50, 1, 0)               1 3.5645554065704346\n",
            "(105, Ch2, 50, 1, 0)               0               0\n",
            "(105, Ch2, 50, 1, 0)               1 1.5538461208343506\n",
            "(121, Ch1, 120, 1, 0)               0               0\n",
            "(121, Ch1, 120, 1, 0)               1 13.233757019042969\n",
            "(121, Ch2, 120, 1, 0)               0               0\n",
            "(121, Ch2, 120, 1, 0)               1 10.245112419128418\n",
            "(126, Ch1, 50, 1, 0)               0               0\n",
            "(126, Ch1, 50, 1, 0)               1 -2.7612056732177734\n",
            "(126, Ch2, 50, 1, 0)               0               0\n",
            "(126, Ch2, 50, 1, 0)               1 -3.9951319694519043\n",
            "(129, Ch1, 50, 1, 0)               0               0\n",
            "(129, Ch1, 50, 1, 0)               1 1.986384630203247\n",
            "(129, Ch2, 50, 1, 0)               0               0\n",
            "(129, Ch2, 50, 1, 0)               1 -0.11810547113418579\n",
            "(132, Ch1, 50, 1, 0)               0               0\n",
            "(132, Ch1, 50, 1, 0)               1 3.1254427433013916\n",
            "(132, Ch2, 50, 1, 0)               0               0\n",
            "(132, Ch2, 50, 1, 0)               1 1.5612351894378662\n",
            "(135, Ch1, 50, 1, 0)               0               0\n",
            "(135, Ch1, 50, 1, 0)               1 4.561314105987549\n",
            "(135, Ch2, 50, 1, 0)               0               0\n",
            "(135, Ch2, 50, 1, 0)               1 2.0485992431640625\n",
            "******************************************************************\n",
            "(31, Ch1, 120, 1, 0) Send Back       13.735158920288086\n",
            "(31, Ch1, 120, 1, 0) Send Back       13.735158920288086\n",
            "(31, Ch2, 120, 1, 0) Send Back       12.685922622680664\n",
            "(31, Ch2, 120, 1, 0) Send Back       12.685922622680664\n",
            "(36, Ch1, 50, 1, 0) Send Back       5.22218132019043\n",
            "(36, Ch1, 50, 1, 0) Send Back       5.22218132019043\n",
            "(36, Ch2, 50, 1, 0) Send Back       0.797095775604248\n",
            "(36, Ch2, 50, 1, 0) Send Back       0.797095775604248\n",
            "(39, Ch1, 50, 1, 0) Send Back       7.4098968505859375\n",
            "(39, Ch1, 50, 1, 0) Send Back       7.4098968505859375\n",
            "(39, Ch2, 50, 1, 0) Send Back       7.55939245223999\n",
            "(39, Ch2, 50, 1, 0) Send Back       7.55939245223999\n",
            "(42, Ch1, 50, 1, 0) Send Back       6.192809104919434\n",
            "(42, Ch1, 50, 1, 0) Send Back       6.192809104919434\n",
            "(42, Ch2, 50, 1, 0) Send Back       8.644883155822754\n",
            "(42, Ch2, 50, 1, 0) Send Back       8.644883155822754\n",
            "(45, Ch1, 50, 1, 0) Send Back       8.134713172912598\n",
            "(45, Ch1, 50, 1, 0) Send Back       8.134713172912598\n",
            "(45, Ch2, 50, 1, 0) Send Back       8.475969314575195\n",
            "(45, Ch2, 50, 1, 0) Send Back       8.475969314575195\n",
            "(61, Ch1, 120, 1, 0) Send Back       14.871761322021484\n",
            "(61, Ch1, 120, 1, 0) Send Back       14.871761322021484\n",
            "(61, Ch2, 120, 1, 0) Send Back       12.297138214111328\n",
            "(61, Ch2, 120, 1, 0) Send Back       12.297138214111328\n",
            "(66, Ch1, 50, 1, 0) Wait                          0\n",
            "(66, Ch1, 50, 1, 0) Wait                          0\n",
            "(66, Ch2, 50, 1, 0) Wait                          0\n",
            "(66, Ch2, 50, 1, 0) Wait                          0\n",
            "(69, Ch1, 50, 1, 0) Send Back       3.7653908729553223\n",
            "(69, Ch1, 50, 1, 0) Send Back       3.7653908729553223\n",
            "(69, Ch2, 50, 1, 0) Send Back       1.2179077863693237\n",
            "(69, Ch2, 50, 1, 0) Send Back       1.2179077863693237\n",
            "(72, Ch1, 50, 1, 0) Send Back       1.8534140586853027\n",
            "(72, Ch1, 50, 1, 0) Send Back       1.8534140586853027\n",
            "(72, Ch2, 50, 1, 0) Send Back       2.354914665222168\n",
            "(72, Ch2, 50, 1, 0) Send Back       2.354914665222168\n",
            "(75, Ch1, 50, 1, 0) Send Back       4.470059394836426\n",
            "(75, Ch1, 50, 1, 0) Send Back       4.470059394836426\n",
            "(75, Ch2, 50, 1, 0) Send Back       2.8846640586853027\n",
            "(75, Ch2, 50, 1, 0) Send Back       2.8846640586853027\n",
            "(91, Ch1, 120, 1, 0) Send Back       10.2926607131958\n",
            "(91, Ch1, 120, 1, 0) Send Back       10.2926607131958\n",
            "(91, Ch2, 120, 1, 0) Send Back       8.920186996459961\n",
            "(91, Ch2, 120, 1, 0) Send Back       8.920186996459961\n",
            "(96, Ch1, 50, 1, 0) Wait                          0\n",
            "(96, Ch1, 50, 1, 0) Wait                          0\n",
            "(96, Ch2, 50, 1, 0) Wait                          0\n",
            "(96, Ch2, 50, 1, 0) Wait                          0\n",
            "(99, Ch1, 50, 1, 0) Send Back       2.88713002204895\n",
            "(99, Ch1, 50, 1, 0) Send Back       2.88713002204895\n",
            "(99, Ch2, 50, 1, 0) Send Back       1.8777236938476562\n",
            "(99, Ch2, 50, 1, 0) Send Back       1.8777236938476562\n",
            "(102, Ch1, 50, 1, 0) Send Back       1.5050315856933594\n",
            "(102, Ch1, 50, 1, 0) Send Back       1.5050315856933594\n",
            "(102, Ch2, 50, 1, 0) Send Back       0.3621219992637634\n",
            "(102, Ch2, 50, 1, 0) Send Back       0.3621219992637634\n",
            "(105, Ch1, 50, 1, 0) Send Back       3.5645554065704346\n",
            "(105, Ch1, 50, 1, 0) Send Back       3.5645554065704346\n",
            "(105, Ch2, 50, 1, 0) Send Back       1.5538461208343506\n",
            "(105, Ch2, 50, 1, 0) Send Back       1.5538461208343506\n",
            "(121, Ch1, 120, 1, 0) Send Back       13.233757019042969\n",
            "(121, Ch1, 120, 1, 0) Send Back       13.233757019042969\n",
            "(121, Ch2, 120, 1, 0) Send Back       10.245112419128418\n",
            "(121, Ch2, 120, 1, 0) Send Back       10.245112419128418\n",
            "(126, Ch1, 50, 1, 0) Wait                          0\n",
            "(126, Ch1, 50, 1, 0) Wait                          0\n",
            "(126, Ch2, 50, 1, 0) Wait                          0\n",
            "(126, Ch2, 50, 1, 0) Wait                          0\n",
            "(129, Ch1, 50, 1, 0) Send Back       1.986384630203247\n",
            "(129, Ch1, 50, 1, 0) Send Back       1.986384630203247\n",
            "(129, Ch2, 50, 1, 0) Wait                          0\n",
            "(129, Ch2, 50, 1, 0) Wait                          0\n",
            "(132, Ch1, 50, 1, 0) Send Back       3.1254427433013916\n",
            "(132, Ch1, 50, 1, 0) Send Back       3.1254427433013916\n",
            "(132, Ch2, 50, 1, 0) Send Back       1.5612351894378662\n",
            "(132, Ch2, 50, 1, 0) Send Back       1.5612351894378662\n",
            "(135, Ch1, 50, 1, 0) Send Back       4.561314105987549\n",
            "(135, Ch1, 50, 1, 0) Send Back       4.561314105987549\n",
            "(135, Ch2, 50, 1, 0) Send Back       2.0485992431640625\n",
            "(135, Ch2, 50, 1, 0) Send Back       2.0485992431640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "episode = 0\n",
        "Lesson_buffer_a1 = LessonBuffer(1000, 25, 5)\n",
        "Lesson_buffer_a0 = LessonBuffer(1000, 25, 5)\n",
        "\n",
        "rudder_lstm_a0 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a0, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "rudder_lstm_a1 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a1, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "rudder_lstm_a0.load_state_dict(torch.load('rudder_lstm_wait_100.pt'))\n",
        "rudder_lstm_a1.load_state_dict(torch.load('rudder_lstm_send_100.pt'))\n",
        "\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "for i in range(5000):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "\n",
        "    while not done:\n",
        "        if episode <= 24:\n",
        "          action = np.random.choice(2)\n",
        "        else:\n",
        "          if np.random.random() < 0.15:\n",
        "              action = np.random.choice(2) \n",
        "          else:\n",
        "              action = 0 if policy_updator.Quality[name,0] > policy_updator.Quality[name,1] else 1\n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "    \n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            if actions[0] == 0: \n",
        "              rewards = rudder_lstm_a0.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            if actions[0] == 1: \n",
        "              rewards = rudder_lstm_a1.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            \n",
        "            policy_updator.Q_learning(actions= actions , states = states, rewards= rewards)\n",
        "\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            print('{:15} {:15} {:15}'.format( keys[0] ,  keys[1], value))\n",
        "print(\"******************************************************************\")\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))"
      ],
      "metadata": {
        "id": "alRHIJG-dAQx",
        "outputId": "1575139b-bd38-4bd1-8037-d83f62af2ff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.75, 0.25], [0.85, 0.15000000000000002]]\n",
            "Amir\n",
            "(32, Ch1, 100, 1, 0)               0 1.8994104862213135\n",
            "(32, Ch1, 100, 1, 0)               1 0.748117208480835\n",
            "(32, Ch2, 100, 1, 0)               0 2.4019837379455566\n",
            "(32, Ch2, 100, 1, 0)               1 -1.077941656112671\n",
            "(35, Ch1, 100, 1, 0)               0 1.869750738143921\n",
            "(35, Ch1, 100, 1, 0)               1 1.4833705425262451\n",
            "(35, Ch2, 100, 1, 0)               0 3.6602396965026855\n",
            "(35, Ch2, 100, 1, 0)               1 0.04431354999542236\n",
            "(62, Ch1, 100, 1, 0)               0 1.4699854850769043\n",
            "(62, Ch1, 100, 1, 0)               1 0.6710265278816223\n",
            "(62, Ch2, 100, 1, 0)               0 1.478189468383789\n",
            "(62, Ch2, 100, 1, 0)               1 -0.7483839988708496\n",
            "(65, Ch1, 100, 1, 0)               0 1.4782333374023438\n",
            "(65, Ch1, 100, 1, 0)               1 3.5302302837371826\n",
            "(65, Ch2, 100, 1, 0)               0 1.7438583374023438\n",
            "(65, Ch2, 100, 1, 0)               1 0.9370671510696411\n",
            "(92, Ch1, 100, 1, 0)               0 1.470623254776001\n",
            "(92, Ch1, 100, 1, 0)               1 1.1598742008209229\n",
            "(92, Ch2, 100, 1, 0)               0 4.508622646331787\n",
            "(92, Ch2, 100, 1, 0)               1 -0.8383942246437073\n",
            "(95, Ch1, 100, 1, 0)               0 1.4672787189483643\n",
            "(95, Ch1, 100, 1, 0)               1 1.168839454650879\n",
            "(95, Ch2, 100, 1, 0)               0 1.7010667324066162\n",
            "(95, Ch2, 100, 1, 0)               1 -0.7315077185630798\n",
            "(122, Ch1, 100, 1, 0)               0 1.469710350036621\n",
            "(122, Ch1, 100, 1, 0)               1 1.3466308116912842\n",
            "(122, Ch2, 100, 1, 0)               0 4.2873969078063965\n",
            "(122, Ch2, 100, 1, 0)               1 -1.0492048263549805\n",
            "(125, Ch1, 100, 1, 0)               0 1.4672393798828125\n",
            "(125, Ch1, 100, 1, 0)               1 1.3080430030822754\n",
            "(125, Ch2, 100, 1, 0)               0 6.2484025955200195\n",
            "(125, Ch2, 100, 1, 0)               1 -0.48459237813949585\n",
            "******************************************************************\n",
            "(32, Ch1, 100, 1, 0) Wait            1.8994104862213135\n",
            "(32, Ch1, 100, 1, 0) Wait            1.8994104862213135\n",
            "(32, Ch2, 100, 1, 0) Wait            2.4019837379455566\n",
            "(32, Ch2, 100, 1, 0) Wait            2.4019837379455566\n",
            "(35, Ch1, 100, 1, 0) Wait            1.869750738143921\n",
            "(35, Ch1, 100, 1, 0) Wait            1.869750738143921\n",
            "(35, Ch2, 100, 1, 0) Wait            3.6602396965026855\n",
            "(35, Ch2, 100, 1, 0) Wait            3.6602396965026855\n",
            "(62, Ch1, 100, 1, 0) Wait            1.4699854850769043\n",
            "(62, Ch1, 100, 1, 0) Wait            1.4699854850769043\n",
            "(62, Ch2, 100, 1, 0) Wait            1.478189468383789\n",
            "(62, Ch2, 100, 1, 0) Wait            1.478189468383789\n",
            "(65, Ch1, 100, 1, 0) Send Back       3.5302302837371826\n",
            "(65, Ch1, 100, 1, 0) Send Back       3.5302302837371826\n",
            "(65, Ch2, 100, 1, 0) Wait            1.7438583374023438\n",
            "(65, Ch2, 100, 1, 0) Wait            1.7438583374023438\n",
            "(92, Ch1, 100, 1, 0) Wait            1.470623254776001\n",
            "(92, Ch1, 100, 1, 0) Wait            1.470623254776001\n",
            "(92, Ch2, 100, 1, 0) Wait            4.508622646331787\n",
            "(92, Ch2, 100, 1, 0) Wait            4.508622646331787\n",
            "(95, Ch1, 100, 1, 0) Wait            1.4672787189483643\n",
            "(95, Ch1, 100, 1, 0) Wait            1.4672787189483643\n",
            "(95, Ch2, 100, 1, 0) Wait            1.7010667324066162\n",
            "(95, Ch2, 100, 1, 0) Wait            1.7010667324066162\n",
            "(122, Ch1, 100, 1, 0) Wait            1.469710350036621\n",
            "(122, Ch1, 100, 1, 0) Wait            1.469710350036621\n",
            "(122, Ch2, 100, 1, 0) Wait            4.2873969078063965\n",
            "(122, Ch2, 100, 1, 0) Wait            4.2873969078063965\n",
            "(125, Ch1, 100, 1, 0) Wait            1.4672393798828125\n",
            "(125, Ch1, 100, 1, 0) Wait            1.4672393798828125\n",
            "(125, Ch2, 100, 1, 0) Wait            6.2484025955200195\n",
            "(125, Ch2, 100, 1, 0) Wait            6.2484025955200195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Finding Optimal policies with DQN** "
      ],
      "metadata": {
        "id": "UDEUrRs_Ftzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims,\n",
        "                 n_actions):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = T.Tensor(state).to(self.device)\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        actions = self.fc3(x)\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
        "                 max_mem_size=100000, eps_end=0.05, eps_dec=5e-4):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_min = eps_end\n",
        "        self.eps_dec = eps_dec\n",
        "        self.lr = lr\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.mem_size = max_mem_size\n",
        "        self.batch_size = batch_size\n",
        "        self.mem_cntr = 0\n",
        "        self.iter_cntr = 0\n",
        "        self.replace_target = 100\n",
        "\n",
        "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions,\n",
        "                                   input_dims=input_dims,\n",
        "                                   fc1_dims=256, fc2_dims=256)\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_dims),\n",
        "                                     dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_dims),\n",
        "                                         dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, terminal):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.reward_memory[index] = reward\n",
        "        self.action_memory[index] = action\n",
        "        self.terminal_memory[index] = terminal\n",
        "\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            # state = T.tensor(observation).to(self.Q_eval.device)      \n",
        "            actions = self.Q_eval.forward(observation)\n",
        "            action = T.argmax(actions).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if self.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
        "        new_state_batch = T.tensor(\n",
        "                self.new_state_memory[batch]).to(self.Q_eval.device)\n",
        "        action_batch = self.action_memory[batch]\n",
        "        reward_batch = T.tensor(\n",
        "                self.reward_memory[batch]).to(self.Q_eval.device)\n",
        "        terminal_batch = T.tensor(\n",
        "                self.terminal_memory[batch]).to(self.Q_eval.device)\n",
        "\n",
        "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
        "        q_next = self.Q_eval.forward(new_state_batch)\n",
        "        q_next[terminal_batch] = 0.0\n",
        "        # print((q_next)[:,0])\n",
        "        # print(T.max(q_next, dim=1)[0])\n",
        "        # print((q_next)[:,0])\n",
        "        # Time.sleep(1)\n",
        "        q_target = reward_batch + self.gamma*T.max(q_next, dim=1)[0]\n",
        "\n",
        "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "\n",
        "        self.iter_cntr += 1\n",
        "        self.epsilon = self.epsilon - self.eps_dec \\\n",
        "            if self.epsilon > self.eps_min else self.eps_min"
      ],
      "metadata": {
        "id": "XmayvH3QF-zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpBfjlvf-Gwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "from tqdm import tqdm\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "episode = 0\n",
        "\n",
        "Lesson_buffer_a1 = LessonBuffer(1000, 25, 5)\n",
        "Lesson_buffer_a0 = LessonBuffer(1000, 25, 5)\n",
        "\n",
        "rudder_lstm_a0 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a0, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "rudder_lstm_a1 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a1, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "rudder_lstm_a0.load_state_dict(torch.load('rudder_lstm_wait_50_120.pt'))\n",
        "rudder_lstm_a1.load_state_dict(torch.load('rudder_lstm_send_50_120.pt'))\n",
        "\n",
        "\n",
        "DQN_Agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=2, eps_end=0.01,\n",
        "                  input_dims=[5], lr=0.001)\n",
        "\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "for i in tqdm(range(2000)):\n",
        "    # print(episode)\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    dones = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "    # prev_state = state\n",
        "\n",
        "    initial_state= state\n",
        "    if initial_state[1] == \"Ch1\":\n",
        "        initial_state[1] = 1\n",
        "    else:\n",
        "        initial_state[1] = 0\n",
        "    initial_state = initial_state.astype(int)\n",
        "\n",
        "    while not done:\n",
        "        if episode <= 50:\n",
        "          action = np.random.choice(2)\n",
        "        else:\n",
        "          if np.random.random() < 0.15:\n",
        "              action = np.random.choice(2) \n",
        "          else:      \n",
        "              action = DQN_Agent.choose_action(initial_state)\n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "\n",
        "        # prev_state = state\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        dones.append(done)\n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            if actions[0] == 0: \n",
        "              rewards = rudder_lstm_a0.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            if actions[0] == 1: \n",
        "              rewards = rudder_lstm_a1.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            for i in range(2):\n",
        "              DQN_Agent.store_transition(states[i], actions[i], rewards[i], states[i+1],\n",
        "                                  dones[i])\n",
        "              DQN_Agent.learn()\n",
        "\n",
        "for state in environment.initial_State:\n",
        "  State  =  np.array([state.Au, state.Ch, state.BT, state.Ra, state.U])\n",
        "  if State[1] == \"Ch1\":\n",
        "    State[1] = 1\n",
        "  else:\n",
        "    State[1] = 0\n",
        "  State = State.astype(int) \n",
        "  print(f'{State} ___________________________ {DQN_Agent.Q_eval.forward(State)}')\n",
        "# for keys, value in policy_updator.Quality.items():\n",
        "#          initial_StateName = []\n",
        "#          for i in environment.initial_State:\n",
        "#             initial_StateName.append(i.Name) \n",
        "#          if keys[0] in initial_StateName: \n",
        "#             print('{:15} {:15} {:15}'.format( keys[0] ,  keys[1], value))\n",
        "# print(\"******************************************************************\")\n",
        "# for keys, value in policy_updator.Quality.items():\n",
        "#          initial_StateName = []\n",
        "#          for i in environment.initial_State:\n",
        "#             initial_StateName.append(i.Name) \n",
        "#          if keys[0] in initial_StateName: \n",
        "#             if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "#               print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "#             else:\n",
        "#               print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))"
      ],
      "metadata": {
        "id": "qAkOBqt3F6JY",
        "outputId": "8206c565-94f1-46bd-fd0c-00e7aacc277d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-0ecc943fb23b>:58: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.75, 0.25], [0.85, 0.15000000000000002]]\n",
            "Amir\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2000/2000 [00:38<00:00, 51.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 31   1 120   1   0] ___________________________ tensor([42.3792, 33.4138], grad_fn=<AddBackward0>)\n",
            "[ 31   0 120   1   0] ___________________________ tensor([39.6162, 31.9886], grad_fn=<AddBackward0>)\n",
            "[ 32   1 100   1   0] ___________________________ tensor([37.1836, 27.6543], grad_fn=<AddBackward0>)\n",
            "[ 32   0 100   1   0] ___________________________ tensor([35.2948, 26.6085], grad_fn=<AddBackward0>)\n",
            "[ 35   1 100   1   0] ___________________________ tensor([37.2084, 27.3762], grad_fn=<AddBackward0>)\n",
            "[ 35   0 100   1   0] ___________________________ tensor([35.6811, 26.4414], grad_fn=<AddBackward0>)\n",
            "[ 61   1 120   1   0] ___________________________ tensor([42.4473, 31.2946], grad_fn=<AddBackward0>)\n",
            "[ 61   0 120   1   0] ___________________________ tensor([42.2538, 30.9799], grad_fn=<AddBackward0>)\n",
            "[ 62   1 100   1   0] ___________________________ tensor([43.8141, 25.7209], grad_fn=<AddBackward0>)\n",
            "[ 62   0 100   1   0] ___________________________ tensor([42.3948, 25.4378], grad_fn=<AddBackward0>)\n",
            "[ 65   1 100   1   0] ___________________________ tensor([44.4918, 25.6978], grad_fn=<AddBackward0>)\n",
            "[ 65   0 100   1   0] ___________________________ tensor([43.1322, 25.4130], grad_fn=<AddBackward0>)\n",
            "[ 91   1 120   1   0] ___________________________ tensor([56.1309, 30.6166], grad_fn=<AddBackward0>)\n",
            "[ 91   0 120   1   0] ___________________________ tensor([55.2170, 30.5081], grad_fn=<AddBackward0>)\n",
            "[ 92   1 100   1   0] ___________________________ tensor([55.2058, 25.5519], grad_fn=<AddBackward0>)\n",
            "[ 92   0 100   1   0] ___________________________ tensor([55.3751, 25.6360], grad_fn=<AddBackward0>)\n",
            "[ 95   1 100   1   0] ___________________________ tensor([55.2913, 25.3352], grad_fn=<AddBackward0>)\n",
            "[ 95   0 100   1   0] ___________________________ tensor([56.0082, 25.5471], grad_fn=<AddBackward0>)\n",
            "[121   1 120   1   0] ___________________________ tensor([66.1419, 29.3225], grad_fn=<AddBackward0>)\n",
            "[121   0 120   1   0] ___________________________ tensor([66.7452, 29.6545], grad_fn=<AddBackward0>)\n",
            "[122   1 100   1   0] ___________________________ tensor([66.5099, 14.6339], grad_fn=<AddBackward0>)\n",
            "[122   0 100   1   0] ___________________________ tensor([66.5942, 14.6853], grad_fn=<AddBackward0>)\n",
            "[125   1 100   1   0] ___________________________ tensor([66.6168, 12.7531], grad_fn=<AddBackward0>)\n",
            "[125   0 100   1   0] ___________________________ tensor([66.7239, 12.8057], grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**optimal policies with Dueling Double DQN** "
      ],
      "metadata": {
        "id": "_HDAImTqlSfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                    dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                        dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, terminal\n",
        "\n",
        "class DuelingDeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, n_actions, name, input_dims, chkpt_dir):\n",
        "        super(DuelingDeepQNetwork, self).__init__()\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "\n",
        "        self.fc1 = nn.Linear(*input_dims, 512)\n",
        "        self.V = nn.Linear(512, 1)\n",
        "        self.A = nn.Linear(512, n_actions)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        flat1 = F.relu(self.fc1(state))\n",
        "        V = self.V(flat1)\n",
        "        A = self.A(flat1)\n",
        "\n",
        "        return V, A\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class Dueling_DDQN_Agent():\n",
        "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n",
        "                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n",
        "                 replace=1000, chkpt_dir='tmp/dueling_ddqn'):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.lr = lr\n",
        "        self.n_actions = n_actions\n",
        "        self.input_dims = input_dims\n",
        "        self.batch_size = batch_size\n",
        "        self.eps_min = eps_min\n",
        "        self.eps_dec = eps_dec\n",
        "        self.replace_target_cnt = replace\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "        self.action_space = [i for i in range(self.n_actions)]\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "\n",
        "        self.q_eval = DuelingDeepQNetwork(self.lr, self.n_actions,\n",
        "                                   input_dims=self.input_dims,\n",
        "                                   name='lunar_lander_dueling_ddqn_q_eval',\n",
        "                                   chkpt_dir=self.chkpt_dir)\n",
        "\n",
        "        self.q_next = DuelingDeepQNetwork(self.lr, self.n_actions,\n",
        "                                   input_dims=self.input_dims,\n",
        "                                   name='lunar_lander_dueling_ddqn_q_next',\n",
        "                                   chkpt_dir=self.chkpt_dir)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)\n",
        "            _, advantage = self.q_eval.forward(state)\n",
        "            action = T.argmax(advantage).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        self.memory.store_transition(state, action, reward, state_, done)\n",
        "\n",
        "    def replace_target_network(self):\n",
        "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
        "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
        "\n",
        "    def decrement_epsilon(self):\n",
        "        self.epsilon = self.epsilon - self.eps_dec \\\n",
        "                        if self.epsilon > self.eps_min else self.eps_min\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_eval.save_checkpoint()\n",
        "        self.q_next.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.q_eval.load_checkpoint()\n",
        "        self.q_next.load_checkpoint()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.q_eval.optimizer.zero_grad()\n",
        "\n",
        "        self.replace_target_network()\n",
        "\n",
        "        state, action, reward, new_state, done = \\\n",
        "                                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        states = T.tensor(state).to(self.q_eval.device)\n",
        "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
        "        dones = T.tensor(done).to(self.q_eval.device)\n",
        "        actions = T.tensor(action).to(self.q_eval.device)\n",
        "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
        "\n",
        "        indices = np.arange(self.batch_size)\n",
        "\n",
        "        V_s, A_s = self.q_eval.forward(states)\n",
        "        V_s_, A_s_ = self.q_next.forward(states_)\n",
        "\n",
        "        V_s_eval, A_s_eval = self.q_eval.forward(states_)\n",
        "\n",
        "        q_pred = T.add(V_s,\n",
        "                        (A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions]\n",
        "        q_next = T.add(V_s_,\n",
        "                        (A_s_ - A_s_.mean(dim=1, keepdim=True)))\n",
        "\n",
        "        q_eval = T.add(V_s_eval, (A_s_eval - A_s_eval.mean(dim=1,keepdim=True)))\n",
        "\n",
        "        max_actions = T.argmax(q_eval, dim=1)\n",
        "        max_actions[indices] = 0\n",
        "        q_next[dones] = 0.0\n",
        "        q_target = rewards + self.gamma*q_next[indices, max_actions]\n",
        "\n",
        "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
        "        loss.backward()\n",
        "        self.q_eval.optimizer.step()\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "        self.decrement_epsilon()"
      ],
      "metadata": {
        "id": "U8f7SPwQlRiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "from tqdm import tqdm\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "episode = 0\n",
        "Lesson_buffer_a1 = LessonBuffer(1000, 25, 5)\n",
        "Lesson_buffer_a0 = LessonBuffer(1000, 25, 5)\n",
        "\n",
        "rudder_lstm_a0 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a0, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "rudder_lstm_a1 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a1, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "Lesson_buffer_a1_100 = LessonBuffer(1000, 25, 5)\n",
        "Lesson_buffer_a0_100 = LessonBuffer(1000, 25, 5)\n",
        "\n",
        "rudder_lstm_a0_100 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a0_100, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "rudder_lstm_a1_100 = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer_a1_100, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "\n",
        "rudder_lstm_a0.load_state_dict(torch.load('rudder_lstm_wait_120.pt'))\n",
        "rudder_lstm_a1.load_state_dict(torch.load('rudder_lstm_send_120.pt'))\n",
        "rudder_lstm_a0_100.load_state_dict(torch.load('rudder_lstm_wait_100.pt'))\n",
        "rudder_lstm_a1_100.load_state_dict(torch.load('rudder_lstm_send_100.pt'))\n",
        "\n",
        "\n",
        "Dueling_Double_DQN_Agent = Dueling_DDQN_Agent(gamma=1, epsilon=1.0, lr=3e-4,\n",
        "                  input_dims=[5], n_actions=2, mem_size=100000, eps_min=0.01,\n",
        "                  batch_size=64, eps_dec=1e-3, replace=100)\n",
        "\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "for i in tqdm(range(8000)):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    dones = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "\n",
        "    initial_state= state\n",
        "    if initial_state[1] == \"Ch1\":\n",
        "        initial_state[1] = 1\n",
        "    else:\n",
        "        initial_state[1] = 0\n",
        "    initial_state = initial_state.astype(int)\n",
        "\n",
        "    while not done:\n",
        "        action = Dueling_Double_DQN_Agent.choose_action(initial_state)\n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        dones.append(done)\n",
        "        if done: \n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              # print(res)\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            if actions[0] == 0: \n",
        "              if states[0][2] == 120:\n",
        "                  rewards = rudder_lstm_a0.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "              else:\n",
        "                  rewards = rudder_lstm_a0_100.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            if actions[0] == 1: \n",
        "              if states[0][2] == 120:\n",
        "                  rewards = rudder_lstm_a1.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "              else:\n",
        "                  rewards = rudder_lstm_a1_100.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :] \n",
        "            for i in range(25):\n",
        "              Dueling_Double_DQN_Agent.store_transition(states[i], actions[i], rewards[i], states[i+1],\n",
        "                                  dones[i])\n",
        "            Dueling_Double_DQN_Agent.learn()\n",
        "\n",
        "for state in environment.initial_State:\n",
        "  State  =  np.array([state.Au, state.Ch, state.BT, state.Ra, state.U])\n",
        "  if State[1] == \"Ch1\":\n",
        "    State[1] = 1\n",
        "  else:\n",
        "    State[1] = 0\n",
        "  State = State.astype(int) \n",
        "  state = T.tensor([State],dtype=T.float)\n",
        "  _, advantage =  Dueling_Double_DQN_Agent.q_eval.forward(state)\n",
        "  action = T.argmax(advantage).item()\n",
        "\n",
        "  print(f'{State} __________________ {advantage}_________________{action}')\n"
      ],
      "metadata": {
        "id": "ubtzi8mDlrwB",
        "outputId": "2d08e546-703d-45d1-e4bc-a87ac3606a80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.75, 0.25], [0.85, 0.15000000000000002]]\n",
            "Amir\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/8000 [00:00<?, ?it/s]<ipython-input-4-c02116dbfbd1>:162: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)\n",
            "  q_next[dones] = 0.0\n",
            "100%|| 8000/8000 [02:09<00:00, 61.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 31   1 120   1   0] __________________ tensor([[-13.7881, -13.0689]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 31   0 120   1   0] __________________ tensor([[-14.6618, -12.1878]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 32   1 100   1   0] __________________ tensor([[-10.8738, -13.0711]], grad_fn=<AddmmBackward0>)_________________0\n",
            "[ 32   0 100   1   0] __________________ tensor([[-11.6513, -12.3073]], grad_fn=<AddmmBackward0>)_________________0\n",
            "[ 35   1 100   1   0] __________________ tensor([[-11.2669, -13.3000]], grad_fn=<AddmmBackward0>)_________________0\n",
            "[ 35   0 100   1   0] __________________ tensor([[-12.0735, -12.5111]], grad_fn=<AddmmBackward0>)_________________0\n",
            "[ 61   1 120   1   0] __________________ tensor([[-17.1445, -15.9088]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 61   0 120   1   0] __________________ tensor([[-18.5267, -14.5076]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 62   1 100   1   0] __________________ tensor([[-14.4802, -15.2714]], grad_fn=<AddmmBackward0>)_________________0\n",
            "[ 62   0 100   1   0] __________________ tensor([[-15.7042, -14.0197]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 65   1 100   1   0] __________________ tensor([[-14.8231, -15.4577]], grad_fn=<AddmmBackward0>)_________________0\n",
            "[ 65   0 100   1   0] __________________ tensor([[-16.0197, -14.2245]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 91   1 120   1   0] __________________ tensor([[-20.3960, -18.1021]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 91   0 120   1   0] __________________ tensor([[-21.5646, -16.9345]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 92   1 100   1   0] __________________ tensor([[-17.2035, -16.7526]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 92   0 100   1   0] __________________ tensor([[-18.2026, -15.8092]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 95   1 100   1   0] __________________ tensor([[-17.6344, -16.9520]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[ 95   0 100   1   0] __________________ tensor([[-18.6400, -16.0325]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[121   1 120   1   0] __________________ tensor([[-22.6756, -20.3490]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[121   0 120   1   0] __________________ tensor([[-23.6626, -19.4368]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[122   1 100   1   0] __________________ tensor([[-19.1666, -20.4724]], grad_fn=<AddmmBackward0>)_________________0\n",
            "[122   0 100   1   0] __________________ tensor([[-20.0054, -19.6830]], grad_fn=<AddmmBackward0>)_________________1\n",
            "[125   1 100   1   0] __________________ tensor([[-19.3616, -20.8415]], grad_fn=<AddmmBackward0>)_________________0\n",
            "[125   0 100   1   0] __________________ tensor([[-20.1964, -20.0513]], grad_fn=<AddmmBackward0>)_________________1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "\n",
        "Lesson_buffer = LessonBuffer(1000, 25, 5)\n",
        "episode = 0\n",
        "rudder_lstm = LSTM(state_input_size=5, n_actions= 2, buffer=Lesson_buffer, n_units=n_lstm,\n",
        "                        lstm_lr=lstm_lr, l2_regularization=l2_regularization, return_scaling=10,\n",
        "                        lstm_batch_size=8, continuous_pred_factor=0.5)\n",
        "# rudder_lstm.load_state_dict(torch.load('rudder_lstm.pt'))\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "episode = 0\n",
        "for i in range(2050):\n",
        "    episode += 1\n",
        "    environment.reset_paramter()\n",
        "    state = environment.reset_state()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    environment.generate_channel_state_list_for_whole_sequence(state[1])\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({state[0]}, {state[1]}, {state[2]}, {state[3]}, {state[4]})'\n",
        "\n",
        "    while not done:\n",
        "        action = 0 if policy_updator.Quality[name,0] > policy_updator.Quality[name,1] else 1\n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "            action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "            action = 1\n",
        "        if environment.state.U > 0:\n",
        "            action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "\n",
        "            res = np.nonzero(rewards)[0]\n",
        "            if len(res) > 0 :\n",
        "              rewards[-1] = rewards[res[0]]\n",
        "              rewards[res[0]] = 0   \n",
        "            for i in states: \n",
        "                if i[1] == \"Ch1\":\n",
        "                    i[1] = 1\n",
        "                else:\n",
        "                    i[1] = 0\n",
        "            states = np.stack(states)\n",
        "            states = states.astype(int)\n",
        "            rewards = np.array(rewards, dtype = np.float32)\n",
        "            actions = np.array(actions)\n",
        "            Lesson_buffer.add(states = states, actions = actions, rewards = rewards)\n",
        "            if  episode < 5000 and Lesson_buffer.full_enough() and Lesson_buffer.different_returns_encountered()  :\n",
        "                    if episode % 25 == 0:\n",
        "\n",
        "                        print(episode)\n",
        "                        rudder_lstm.train(episode=episode)\n",
        "                    if episode >= 1800: \n",
        "                        torch.save(rudder_lstm.state_dict(), 'rudder_lstm_120.pt')\n",
        "\n",
        "            rewards = rudder_lstm.redistribute_reward(states=np.expand_dims(states, 0),actions=np.expand_dims(actions, 0))[0, :]\n",
        "            policy_updator.Q_learning(actions= actions , states = states, rewards= rewards)\n",
        "\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            print('{:15} {:15} {:15}'.format( keys[0] ,  keys[1], value))\n",
        "\n",
        "print(\"******************************************************************\")\n",
        "Optimal_Policy_Dict = {}\n",
        "for keys, value in policy_updator.Quality.items():\n",
        "         initial_StateName = []\n",
        "         for i in environment.initial_State:\n",
        "            initial_StateName.append(i.Name) \n",
        "         if keys[0] in initial_StateName: \n",
        "            if policy_updator.Quality[keys[0],0] > policy_updator.Quality[keys[0],1]:  \n",
        "              Optimal_Policy_Dict[keys[0]] = \"wait\"\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Wait\" , policy_updator.Quality[keys[0] ,0]))\n",
        "            else:\n",
        "              Optimal_Policy_Dict[keys[0]] = \"send\"\n",
        "              print('{:15} {:15} {:15}'.format( keys[0] , \"Send Back\" , policy_updator.Quality[keys[0] ,1]))\n",
        "print(Optimal_Policy_Dict)"
      ],
      "metadata": {
        "id": "dyXz-aBqze-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyQt5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sLI2LocA0LL",
        "outputId": "3404f751-5990-4eec-cc5c-ded99dc779f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyQt5\n",
            "  Downloading PyQt5-5.15.9-cp37-abi3-manylinux_2_17_x86_64.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyQt5-sip<13,>=12.11\n",
            "  Downloading PyQt5_sip-12.11.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (361 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m361.8/361.8 KB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyQt5-Qt5>=5.15.2\n",
            "  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m59.9/59.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyQt5-Qt5, PyQt5-sip, PyQt5\n",
            "Successfully installed PyQt5-5.15.9 PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from Environment import Actor\n",
        "from Rudder import LessonBuffer\n",
        "from Environment import Environment\n",
        "from Rudder import RRLSTM as LSTM\n",
        "import torch\n",
        "import time as Time\n",
        "import random\n",
        "from PolicyUpdater import PolicyUpdater\n",
        "from matplotlib import pyplot as plt\n",
        "from StateDto import State\n",
        "\n",
        "\n",
        "lb_size = 2048\n",
        "n_lstm = 16\n",
        "max_time = 50\n",
        "policy_lr = 0.5\n",
        "lstm_lr = 1e-2\n",
        "l2_regularization = 1e-6\n",
        "avg_window = 750\n",
        "UseOptimalPolicy = True\n",
        "episode = 0\n",
        "NUM_EPISODE = 100\n",
        "environment = Environment(100,25)\n",
        "environment.CreateStates()\n",
        "policy_updator  = PolicyUpdater(environment= environment, lr = policy_lr)\n",
        "Total_Reward_List = []\n",
        "Total_Reward_List_Without_Optimal_Policy = []\n",
        "\n",
        "for i in range(NUM_EPISODE):\n",
        "    environment.reset_paramter()\n",
        "    state , fixed_State = environment.reset_state()\n",
        "    first_state = state\n",
        "    environment.generate_channel_state_list_for_whole_sequence(state[1])\n",
        "    episode += 1\n",
        "    rewards = []\n",
        "    states = [first_state]\n",
        "    Episode_AoI = [first_state[0]]\n",
        "    actions = []\n",
        "    done = False\n",
        "    name = f'({first_state[0]}, {first_state[1]}, {first_state[2]}, {first_state[3]}, {first_state[4]})'\n",
        "\n",
        "    while not done:\n",
        "        action = 0 \n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "          action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "          action = 1\n",
        "        if environment.state.U > 0:\n",
        "          action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "        Episode_AoI.append(state[0])\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "          Episode_AoI = [int(i) for i in Episode_AoI]\n",
        "          Total_Reward_List_Without_Optimal_Policy.append(sum(Episode_AoI)/25)\n",
        "    rewards = []\n",
        "    states = [first_state]\n",
        "    Episode_AoI = [first_state[0]]\n",
        "    actions = []\n",
        "    done = False\n",
        "    environment.reset_paramter()\n",
        "    environment.state = State(name, int(first_state[0]), (first_state[1]), int(first_state[2]), int(first_state[3]), int(first_state[4]))\n",
        "    while not done:\n",
        "        if UseOptimalPolicy:\n",
        "          action  = 0 if Optimal_Policy_Dict[name] == 'wait' else 1 \n",
        "        # else:\n",
        "        #   action = 1 \n",
        "        if environment.state.Ra == 0 and environment.state.U == 0:\n",
        "          action = 0\n",
        "        if environment.state.Ra == 0 and environment.state.U == 24:\n",
        "          action = 1\n",
        "        if environment.state.U > 0:\n",
        "          action = 0\n",
        "        state, reward, done = environment.step(action)\n",
        "\n",
        "        Episode_AoI.append(state[0])\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward) \n",
        "        if done: \n",
        "          Episode_AoI = [int(i) for i in Episode_AoI]\n",
        "          Total_Reward_List.append(sum(Episode_AoI)/25)\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(len(Total_Reward_List)):\n",
        "#   print(f\"{i+1} request arrival ---------------- Average AoI: {Total_Reward_List[i]}\")\n",
        "\n",
        "# for i in range(len(Total_Reward_List_Without_Optimal_Policy)):\n",
        "#   print(f\"{i+1} request arrival ---------------- Average AoI: {Total_Reward_List_Without_Optimal_Policy[i]}\")\n",
        "\n",
        "# for i in range(len(Total_Reward_List_Without_Optimal_Policy)):\n",
        "#   print(f\"{i+1} request arrival ---------------- Average AoI: {Total_Reward_List_Without_Optimal_Policy[i]}\")\n",
        "%matplotlib inline\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "   \n",
        "ax.plot(Total_Reward_List, label ='Using AoI Scheduler')\n",
        "   \n",
        "ax.plot(Total_Reward_List_Without_Optimal_Policy, label ='Always sending back')\n",
        "\n",
        "AoI_Difference_List = [Total_Reward_List_Without_Optimal_Policy[i] - Total_Reward_List[i] for i in range(len(Total_Reward_List_Without_Optimal_Policy)) ]\n",
        "ax.plot(AoI_Difference_List, label ='Average AoI Difference')\n",
        "\n",
        "# ax.legend()\n",
        "  \n",
        "# fig.suptitle(\"\"\"matplotlib.figure.Figure.show()\n",
        "# function Example\\n\\n\"\"\", fontweight =\"bold\") \n",
        "  \n",
        "fig.show() \n",
        "\n",
        "# fig = plt.figure()\n",
        "\n",
        "# plt.plot(Total_Reward_List,'g', Total_Reward_List_Without_Optimal_Policy, 'r')\n",
        "\n",
        "print(sum(Total_Reward_List)/NUM_EPISODE)\n",
        "print(sum(Total_Reward_List_Without_Optimal_Policy)/NUM_EPISODE)\n",
        "print(sum(AoI_Difference_List)/NUM_EPISODE)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Jv-AyVRQx_ZT",
        "outputId": "26472519-be6b-4c73-e216-94a3325f0a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9, 0.09999999999999998], [1.0, 0.0]]\n",
            "Amir\n",
            "46.80960000000001\n",
            "54.57359999999998\n",
            "7.763999999999999\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}